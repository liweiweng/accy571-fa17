{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 Assignment\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import helper\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance\n",
    "from nltk.corpus import reuters\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following problems we will use the NLTK Reuters corpus which contains 10,788 documents which have been classified into 90 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "#Let's take a look at the categories\n",
    "print(reuters.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Find the number of times a particular token occurs\n",
    "\n",
    "Write a function called $\\texttt{token_counter}$ which takes in an nltk corpora and outputs the number of times a particular token occurs in the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_counter(token,corpora):\n",
    "    '''\n",
    "    Inputs\n",
    "    -------\n",
    "    \n",
    "    token: the token to search for\n",
    "    corpora: a list of documents\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    \n",
    "    count: the number of times the token occurs in the corpora\n",
    "    '''\n",
    "    ###BEGIN SOLUTION###\n",
    "    count = corpora.words().count(token)\n",
    "    ###END SOLUTION###\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_to = token_counter('to',reuters)\n",
    "assert_equal(num_to,34035)\n",
    "num_ship = token_counter('ship',reuters)\n",
    "assert_equal(num_ship,116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word', lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Use CountVectorizer to retrieve the corpus vocabulary\n",
    "\n",
    "Write a function called $\\texttt{most_common_words}$ which uses CountVectorizer to return the vocabulary of the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_common_words(corpora):\n",
    "    '''\n",
    "    Inputs\n",
    "    -------\n",
    "\n",
    "    corpora: a list of documents\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    \n",
    "    vocabulary: a Python dictionary containing each word and its count\n",
    "    '''\n",
    "    ###BEGIN SOLUTION###\n",
    "    cv = CountVectorizer(analyzer='word', lowercase=True)\n",
    "    cv.fit_transform(corpora.words())\n",
    "    vocabulary = cv.vocabulary_\n",
    "    ###END SOLUTION###\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = most_common_words(reuters)\n",
    "assert_is_instance(vocabulary,dict)\n",
    "assert_equal(vocabulary['asian'],3384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Reading and Preprocessing Data\n",
    "Read in the [badges dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/badges/badges.info) (it is stored in the same directory as 'badges.data') using which ever method you like (pandas, numpy, built-in python modules). *HINT: I recommend using pandas to read in this **fixed width** dataset.* \n",
    "\n",
    "The first column are the labels we want to predict (the + or - sign); the second column are the names. If you do this step correctly the data will be identical to the first 5 rows below:\n",
    "```\n",
    "[['+', 'Naoki Abe'],\n",
    " ['-', 'Myriam Abramson'],\n",
    " ['+', 'David W. Aha'],\n",
    " ['+', 'Kamal M. Ali'],\n",
    " ['-', 'Eric Allender']]\n",
    "```\n",
    "\n",
    "Depending on how you read in the data. Format your data so that it is in an acceptable format for [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function in in sci-kit learn.\n",
    "\n",
    "Next Split the data set the random_state parameter to be 0, and assign the approriate parameter to set aside 80% of your dataset for training data.\n",
    "\n",
    "Now use the encode your labels using [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder). \n",
    "\n",
    "Next create a [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) model using sci-kit learn's implementation using the default values.\n",
    "\n",
    "For your training features learn the vocabulary and idf and return the term-document matrix. For your testing set **only** transform documents to a document term matrix.\n",
    "\n",
    "Assign your encoded labels to train_y and test_y for the training and testing labels respectively.\n",
    "Assign your document-term matrices to the training and testing document-term matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###BEGIN SOLUTION\n",
    "df = pd.read_fwf('badges.data', names=('class', 'name'))\n",
    "df.dropna(inplace=True)\n",
    "df.head(5).values\n",
    "\n",
    "train, test = train_test_split(df, random_state=0, train_size=.8)\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "train_y = LabelEncoder().fit_transform(train['class'])\n",
    "train_X = tfidf.fit_transform(train['name'])\n",
    "test_y = LabelEncoder().fit_transform(test['class'])\n",
    "test_X = tfidf.transform(test['name'])\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_equal(train_y.tolist(), [0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
    "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
    "       1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
    "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
    "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
    "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
    "       1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 1, 0])\n",
    "\n",
    "assert_equal(test_y.tolist(), [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "       1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Fit a Naive Bayes Multinomial Model to the Badges dataset\n",
    "Use sci-kit learn's implementation of a [naive bayes classifer for multinomial model](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) to fit a model to the Badges dataset. Lastly get predictions of the testing labels and name your predictions *_pred*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###BEGIN SOLUTION\n",
    "model = MultinomialNB()\n",
    "model.fit(train_X, train_y)\n",
    "_pred = model.predict(test_X)\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.81      0.90        58\n",
      "          1       0.08      1.00      0.15         1\n",
      "\n",
      "avg / total       0.98      0.81      0.88        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(_pred, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_equal(_pred.tolist(), [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Using N-Grams to classify badges \n",
    "Use sci-kit learn's  [Count Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Learn the names and transform the training term-document matrix.\n",
    "For the testing names transform into the testing term-document matrix.\n",
    "\n",
    "Next create another Multinomial Naive Bayes model.\n",
    "Fit the model on the  training term-document matrix and training labels.\n",
    "Next make predictions on the testing labels with the testing term-document matrix. store this value as *p*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###BEGIN SOLUTION\n",
    "cv = CountVectorizer()\n",
    "tk_func = cv.build_analyzer()\n",
    "\n",
    "x1 = cv.fit_transform(train['name'])\n",
    "x2 = cv.transform(test['name'])\n",
    "\n",
    "model2 = MultinomialNB()\n",
    "model.fit(x1,train_y)\n",
    "p = model.predict(x2)\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.98      0.88        47\n",
      "          1       0.50      0.08      0.14        12\n",
      "\n",
      "avg / total       0.74      0.80      0.73        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_equal(p.tolist(), [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will use the NLTK Reuters corpus which contains 10,788 documents which have been classified into 90 topics. The documents have file ids (held in the $\\texttt{fileids()}$ method) which denotes if they are training or testing documents. An example is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test/14826'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's look at the file id for the first document\n",
    "reuters.fileids()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Write a function that splits the reuters data into testing and training sets\n",
    "\n",
    "Write a function called $\\texttt{splitter}$ which takes in a corpora, and returns a list of file ids for the training documents, and a list of file ids for the testing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitter(corpora):\n",
    "    \"\"\"\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "\n",
    "    corpora: an nltk corpora\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    trainig_ids: a list of training document file ids\n",
    "    testing_ids: a list of testing document file ids\n",
    "    \"\"\"\n",
    "\n",
    "    ###BEGIN SOLUTION###\n",
    "    training_ids = [i for i in corpora.fileids() if \"train\" in i]\n",
    "    testing_ids = [i for i in reuters.fileids() if \"test\" in i]\n",
    "    ###END SOLUTION###\n",
    "    \n",
    "    return training_ids, testing_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_ids, testing_ids = splitter(reuters)\n",
    "assert_equal(len(training_ids),7769)\n",
    "assert_equal(len(testing_ids),3019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will use the below function, along with the training and testing ids that we found to get the full testing and training datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_categories_from_fileids(corpus, fileids):\n",
    "    \"\"\"\n",
    "    Finds categories for each element of 'fileids'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus.\n",
    "    fileids: A list of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = [sorted(corpus.categories(fileids=f))[0] for f in fileids]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [reuters.raw(fileids=fileid) for fileid in training_ids][:1000]\n",
    "y_train = get_categories_from_fileids(reuters, training_ids)[:1000]\n",
    "\n",
    "\n",
    "X_test = [reuters.raw(fileids=fileid) for fileid in testing_ids]\n",
    "y_test = get_categories_from_fileids(reuters, testing_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7: Fit a naive bayes classifier using different n-grams\n",
    "\n",
    "Write a function called $\\texttt{naivebayes_categories}$ which takes in a training set of data, a lower n-gram range, and an upper n-gram range. Furthermore, use english stop words in the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naivebayes_categories(X_train, y_train, lower, upper):\n",
    "    \"\"\"\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "    X_train: The training data\n",
    "    y_train: The training labels\n",
    "    lower: the lower n_gram range\n",
    "    upper: the upper n_gram range\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    clf: the fitted model\n",
    "    \n",
    "    \"\"\"\n",
    "    ###BEGIN SOLUTION###\n",
    "    tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "    clf = Pipeline(tools)\n",
    "    clf.set_params(cv__stop_words='english',cv__ngram_range=(lower,upper))\n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    ###END SOLUTION###\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = naivebayes_categories(X_train,y_train,1,2)\n",
    "predictions = model.predict(X_test)\n",
    "assert_is_instance(model,Pipeline)\n",
    "assert_equal(model.classes_[0],'acq')\n",
    "assert_equal(predictions[0],'trade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how accurate the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71745611129513087"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
