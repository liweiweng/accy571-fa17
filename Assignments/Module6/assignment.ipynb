{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6 Assignment\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from gensim import corpora, models\n",
    "from nose.tools import assert_equal, assert_almost_equal, assert_is_instance\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"When analyzing large text corpora, trends can appear. These trends can be repeated use of common\n",
    "phrases or terms that are indicative of common underlying themes or topics. For example, books on programming\n",
    "might refer to themes such as human computer interaction, optimization and performance, or identifying\n",
    "and removing error conditions. Finding these common topics can be important for a number of reasons.\n",
    "On the one hand, when they are completely unknown, they can be used to provide new insight into text documents.\n",
    "On the other hand, when they may be partially or even completely unknown, computationally identified topics can\n",
    "provide deeper or more concise insight into the relationship between documents.\n",
    "The process of identifying these common topics is known as topic modeling, which is generally a form\n",
    "of unsupervised learning. As a specific example, consider the twenty newsgroup data that we have analyzed\n",
    "in scikit learn. While there are twenty different newsgroups, it turns out they can be grouped into six related\n",
    "categories: computers, sports, science, politics, religion, and miscellaneous. While we now these topics ahead of\n",
    "time (from the newsgroup titles), we can apply topic modeling to these data to identify the common words or phrases\n",
    "that define these common topics.\n",
    "In the rest of this notebook, we explore the concept of topic modeling. First we will use the scikit learn\n",
    "library to perform topic modeling. We will introduce and use non-negative matrix factorization and Latent Dirichlet\n",
    "allocation. We apply topic modeling to a text classification problem, and also explore the terms that make up\n",
    "identified topics. Finally, we introduce the gensim library, which provides additional techniques for topic modeling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Counting the number of tokens in a document\n",
    "\n",
    "Write a function called $\\texttt{token_counter}$ that tokenizes a document using an nltk.tokenize method and then returns the number of tokens in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_counter(tokenizer,text):\n",
    "    \"\"\"\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "\n",
    "    tokenizer: the nltk.tokenize method used to tokenize the document\n",
    "    text: the document to tokenize\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    num_tokens: the number of tokens found in the text by the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    ###BEGIN SOLUTION###\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    num_tokens = len(tokens)\n",
    "    ###END SOLUTION###\n",
    "    \n",
    "    return num_tokens, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count,tokens = token_counter(WhitespaceTokenizer(),text)\n",
    "assert_equal(tokens[0],'When')\n",
    "assert_equal(272,token_count)\n",
    "token_count,tokens = token_counter(WordPunctTokenizer(),text)\n",
    "assert_equal(312,token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Finding the top collocated words\n",
    "\n",
    "Write a function called $\\texttt{top_collocated}$ which, given a tokenized text, returns the top collocated bi-grams in a tokenized text using PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_collocated(tokenized_text, num_collocations):\n",
    "    \"\"\"\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "\n",
    "    tokenized_text: the tokenized text\n",
    "    num_collocations: integer, the number of top collocated words to return\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    top_collocations: the top collocated words\n",
    "    \"\"\"\n",
    "\n",
    "    ###BEGIN SOLUTION###\n",
    "    finder = BigramCollocationFinder.from_words(tokenized_text)\n",
    "    top_collocations = finder.nbest(BigramAssocMeasures().pmi, num_collocations)\n",
    "    ###END SOLUTION###\n",
    "    \n",
    "    return top_collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = top_collocated(tokens,10)\n",
    "\n",
    "top_c = [('(', 'from'),\n",
    " ('-', 'negative'),\n",
    " (':', 'computers'),\n",
    " ('Dirichlet', 'allocation'),\n",
    " ('Latent', 'Dirichlet'),\n",
    " ('The', 'process'),\n",
    " ('When', 'analyzing'),\n",
    " ('additional', 'techniques'),\n",
    " ('analyzed', 'in'),\n",
    " ('analyzing', 'large')]\n",
    "\n",
    "assert_equal(top_c,top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When analyzing large text corpora, trends can appear. These trends can be repeated use of common phrases or terms that are indicative of common underlying themes or topics. For example, books on programming might refer to themes such as human computer interaction, optimization and performance, or identifying and removing error conditions. Finding these common topics can be important for a number of reasons. On the one hand, when they are completely unknown, they can be used to provide new insight into text documents. On the other hand, when they may be partially or even completely unknown, computationally identified topics can provide deeper or more concise insight into the relationship between documents.\n",
      "The process of identifying these common topics is known as topic modeling, which is generally a form of unsupervised learning. As a specific example, consider the twenty newsgroup data that we have analyzed in scikit learn. While there are twenty different newsgroups, it turns out they can be grouped into six related categories: computers, sports, science, politics, religion, and miscellaneous. While we now these topics ahead of time (from the newsgroup titles), we can apply topic modeling to these data to identify the common words or phrases that define these common topics.\n",
      "In the rest of this notebook, we explore the concept of topic modeling. First we will use the scikit learn library to perform topic modeling. We will introduce and use non-negative matrix factorization and Latent Dirichlet allocation. We apply topic modeling to a text classification problem, and also explore the terms that make up identified topics. Finally, we introduce the gensim library, which provides additional techniques for topic modeling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text taken from \"Introduction to NLP: Topic Modeling\" notebook\n",
    "\n",
    "text = \"\"\"When analyzing large text corpora, trends can appear. These trends can be repeated use of common phrases or terms that are indicative of common underlying themes or topics. For example, books on programming might refer to themes such as human computer interaction, optimization and performance, or identifying and removing error conditions. Finding these common topics can be important for a number of reasons. On the one hand, when they are completely unknown, they can be used to provide new insight into text documents. On the other hand, when they may be partially or even completely unknown, computationally identified topics can provide deeper or more concise insight into the relationship between documents.\n",
    "The process of identifying these common topics is known as topic modeling, which is generally a form of unsupervised learning. As a specific example, consider the twenty newsgroup data that we have analyzed in scikit learn. While there are twenty different newsgroups, it turns out they can be grouped into six related categories: computers, sports, science, politics, religion, and miscellaneous. While we now these topics ahead of time (from the newsgroup titles), we can apply topic modeling to these data to identify the common words or phrases that define these common topics.\n",
    "In the rest of this notebook, we explore the concept of topic modeling. First we will use the scikit learn library to perform topic modeling. We will introduce and use non-negative matrix factorization and Latent Dirichlet allocation. We apply topic modeling to a text classification problem, and also explore the terms that make up identified topics. Finally, we introduce the gensim library, which provides additional techniques for topic modeling.\n",
    "\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Preprocessing Text\n",
    "\n",
    "For this problem take the text from above and replace all instances of '\\n' with nothing. Next convert the the text above to a list using a period as the delimiter.\n",
    "\n",
    "If done correctly the first 5 items in data will look like this:\n",
    "```\n",
    "['When analyzing large text corpora, trends can appear',\n",
    " ' These trends can be repeated use of common phrases or terms that are indicative of common underlying themes or topics',\n",
    " ' For example, books on programming might refer to themes such as human computer interaction, optimization and performance, or identifying and removing error conditions',\n",
    " ' Finding these common topics can be important for a number of reasons',\n",
    " ' On the one hand, when they are completely unknown, they can be used to provide new insight into text documents',\n",
    " ' On the other hand, when they may be partially or even completely unknown, computationally identified topics can provide deeper or more concise insight into the relationship between documents']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###BEGIN SOLUTION\n",
    "data = text.replace(\"\\n\",\"\").split(\".\")\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When analyzing large text corpora, trends can appear', ' These trends can be repeated use of common phrases or terms that are indicative of common underlying themes or topics', ' For example, books on programming might refer to themes such as human computer interaction, optimization and performance, or identifying and removing error conditions', ' Finding these common topics can be important for a number of reasons', ' On the one hand, when they are completely unknown, they can be used to provide new insight into text documents', ' On the other hand, when they may be partially or even completely unknown, computationally identified topics can provide deeper or more concise insight into the relationship between documents', 'The process of identifying these common topics is known as topic modeling, which is generally a form of unsupervised learning', ' As a specific example, consider the twenty newsgroup data that we have analyzed in scikit learn', ' While there are twenty different newsgroups, it turns out they can be grouped into six related categories: computers, sports, science, politics, religion, and miscellaneous', ' While we now these topics ahead of time (from the newsgroup titles), we can apply topic modeling to these data to identify the common words or phrases that define these common topics', 'In the rest of this notebook, we explore the concept of topic modeling', ' First we will use the scikit learn library to perform topic modeling', ' We will introduce and use non-negative matrix factorization and Latent Dirichlet allocation', ' We apply topic modeling to a text classification problem, and also explore the terms that make up identified topics', ' Finally, we introduce the gensim library, which provides additional techniques for topic modeling', '']\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_equal(data, \n",
    "['When analyzing large text corpora, trends can appear', ' These trends can be repeated use of common phrases or terms that are indicative of common underlying themes or topics', ' For example, books on programming might refer to themes such as human computer interaction, optimization and performance, or identifying and removing error conditions', ' Finding these common topics can be important for a number of reasons', ' On the one hand, when they are completely unknown, they can be used to provide new insight into text documents', ' On the other hand, when they may be partially or even completely unknown, computationally identified topics can provide deeper or more concise insight into the relationship between documents', 'The process of identifying these common topics is known as topic modeling, which is generally a form of unsupervised learning', ' As a specific example, consider the twenty newsgroup data that we have analyzed in scikit learn', ' While there are twenty different newsgroups, it turns out they can be grouped into six related categories: computers, sports, science, politics, religion, and miscellaneous', ' While we now these topics ahead of time (from the newsgroup titles), we can apply topic modeling to these data to identify the common words or phrases that define these common topics', 'In the rest of this notebook, we explore the concept of topic modeling', ' First we will use the scikit learn library to perform topic modeling', ' We will introduce and use non-negative matrix factorization and Latent Dirichlet allocation', ' We apply topic modeling to a text classification problem, and also explore the terms that make up identified topics', ' Finally, we introduce the gensim library, which provides additional techniques for topic modeling', '']\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Creating a vectore space model with Gensim\n",
    "For this problem create a set of stop words by reading in data from 'english.txt' in the same directory and storing it in a set.\n",
    "\n",
    "Next parse text from the variable data and remove stop words (make all words in the sentence lower case). *Label this as txts (we will use it in the next problem).*\n",
    "\n",
    "Then remove words appearing more than once. Now grab each word that eppears more than once (these are our tokens).\n",
    "\n",
    "Now create a dictionary mapping for our text corupus and convert the collection of words in our corpus to a bag of words.\n",
    "\n",
    "Next calculate the inverse document counts for all terms using the gensim's implementation of Tf-idf using on the bag of words and transform the bag of words into the tfidf space.\n",
    "\n",
    "Lastly create an LDA model for our corpus using the  LdaModel implementation in gensim. The random_state should be 0, id2word should the dictionary mapping of ids to words, the number of iterations shoulde be 10000, and the corpus should be the bag of words in the tfidf space. *Name this model: lda_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###BEGIN SOLUTION\n",
    "stop_words = set(pd.read_csv('english.txt').values.flatten().tolist()) # stop words\n",
    "\n",
    "# removing stop words...\n",
    "txts = [[word for word in sentance.lower().split() if word not in stop_words]\n",
    "        for sentance in data]\n",
    "\n",
    "# removing words with frequency > 1\n",
    "frequency = Counter([word for txt in txts for word in txt])\n",
    "tokens = [[token for token in txt if frequency[token] > 1]\n",
    "          for txt in txts]\n",
    "\n",
    "# dictionary mapping and creating bag of words\n",
    "dict_gensim = corpora.Dictionary(tokens)\n",
    "crps = [dict_gensim.doc2bow(txt) for txt in txts]\n",
    "\n",
    "\n",
    "tfidf = models.TfidfModel(crps)\n",
    "crps_tfidf = tfidf[crps]\n",
    "\n",
    "lda_model = models.LdaModel(corpus=crps_tfidf, id2word=dict_gensim, random_state=0, iterations=10000)\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6,\n",
       "  '0.033*\"data\" + 0.033*\"learn\" + 0.033*\"identified\" + 0.033*\"topic\" + 0.033*\"twenty\" + 0.033*\"newsgroup\" + 0.033*\"insight\" + 0.033*\"scikit\" + 0.033*\"will\" + 0.033*\"modeling\"'),\n",
       " (85,\n",
       "  '0.033*\"data\" + 0.033*\"learn\" + 0.033*\"identified\" + 0.033*\"topic\" + 0.033*\"twenty\" + 0.033*\"newsgroup\" + 0.033*\"insight\" + 0.033*\"scikit\" + 0.033*\"will\" + 0.033*\"modeling\"'),\n",
       " (15,\n",
       "  '0.033*\"data\" + 0.033*\"learn\" + 0.033*\"identified\" + 0.033*\"topic\" + 0.033*\"twenty\" + 0.033*\"newsgroup\" + 0.033*\"insight\" + 0.033*\"scikit\" + 0.033*\"will\" + 0.033*\"modeling\"'),\n",
       " (82,\n",
       "  '0.033*\"data\" + 0.033*\"learn\" + 0.033*\"identified\" + 0.033*\"topic\" + 0.033*\"twenty\" + 0.033*\"newsgroup\" + 0.033*\"insight\" + 0.033*\"scikit\" + 0.033*\"will\" + 0.033*\"modeling\"'),\n",
       " (51,\n",
       "  '0.033*\"data\" + 0.033*\"learn\" + 0.033*\"identified\" + 0.033*\"topic\" + 0.033*\"twenty\" + 0.033*\"newsgroup\" + 0.033*\"insight\" + 0.033*\"scikit\" + 0.033*\"will\" + 0.033*\"modeling\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the topics our model selected.\n",
    "lda_model.print_topics(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_equal(lda_model.iterations, 10000)\n",
    "assert_equal(lda_model.decay, .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Using Word2Vec\n",
    "\n",
    "For this problem create a Word2Vec model using gensim's implementation. \n",
    "Pass in the sentences without stop words from the previous problem. Set the maximum distance between the current and predicited word within a sentence to be 5. Ignore all words with a total frequency less than 1. Assign the argument that controls random number generator to be 0, and set the number of iterations over the corpus to be 100. Name this the Word2Vec model to be *model*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###BEGIN SOLUTION\n",
    "model = models.Word2Vec(txts, window=2, min_count=1, seed=1, iter=100)\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_equal(model.alpha, .025)\n",
    "assert_equal(model.batch_words, 10000)\n",
    "assert_equal(model.train_count, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between scikit and learn is 0.913426512011\n",
      "Similarity between corpora and text is 0.864464308736\n",
      "Similarity between text and text is 1.0\n"
     ]
    }
   ],
   "source": [
    "ans1 = model.similarity('scikit', 'learn')\n",
    "ans3 = model.similarity('corpora,', 'text')\n",
    "ans4 = model.similarity('text', 'text')\n",
    "print(\"Similarity between %s and %s is %s\"%('scikit', 'learn', ans1) )\n",
    "print(\"Similarity between %s and %s is %s\"%('corpora', 'text', ans3) )\n",
    "print(\"Similarity between %s and %s is %s\"%('text', 'text', ans4) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Computing path similarity\n",
    "\n",
    "Write a function called $\\texttt{get_path_similarity}$ that takes in two words and calculates their path similarity using the wordnet corpus. Note that the wordnet corpus has been imported above as $\\texttt{wn}$. Recall also that words passed to wordnet have an ending indicated their part of speech. In this case, we will use words that are marked with $\\texttt{.n.01}$ See the lesson notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_path_similarity(x,y):\n",
    "    \"\"\"\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "\n",
    "    x: the first word\n",
    "    y: the second word\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    similarity: the path similarity between the two words\n",
    "    \"\"\"\n",
    "\n",
    "    ###BEGIN SOLUTION###\n",
    "    first_word = x+\".n.01\"\n",
    "    second_word = y+\".n.01\"\n",
    "    w1 = wn.synset(first_word)\n",
    "    w2 = wn.synset(second_word)\n",
    "    similarity = wn.path_similarity(w1, w2)\n",
    "    ###END SOLUTION###\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_almost_equal(get_path_similarity('dog','boy'),0.14285714285714285)\n",
    "assert_almost_equal(get_path_similarity('drive','boy'),0.08333333333333333)\n",
    "assert_almost_equal(get_path_similarity('man','boy'),0.3333333333333333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
