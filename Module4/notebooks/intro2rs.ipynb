{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recommender Systems\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this IPython Notebook, we introduce the concept of a [Recommender System][rs]. Recommender systems are among the most popular machine learning techniques, and, in general, encompass a number of different types of machine learning algorithms that we have covered including classification, regression, and clustering. Some of the most popular recommender system approaches, however, are different and are fundamentally tied to data management. The classic example of a recommender system, is the market basket analysis, where data mining is performed on purchase transaction logs to first learn what types of objects are bought together (e.g., peanut butter and jelly) in order to, second, predict what someone might be interested in buying once they have selected an item (e.g., when peanut butter has been added to the cart, recommend jelly).\n",
    "\n",
    "More formally, the data structures that support this type  of analysis are known as frequent sets, since we collect sets of frequently purchased items. The most famous algorithm in this category is the [_a priori_][wap] algorithm, where we use what we have learned from previous transactions to make predictions before (i.e., _a priori_ information) someone completes a transaction. More generally, these algorithms are known as _collaborative filtering_, since the algorithm collaboratively filters through records of many individuals to identify trends. Formally, these types of algorithms are used to make recommendations, hence the name _recommender systems_. In addition to the traditional _market basket_ analysis, these algorithms are also used to make recommendations for video, such as Netflix or Hulu, and audio sites, such as Pandora or Spotify.\n",
    "\n",
    "In fact, anyone who has shopped online has been exposed to these algorithms. For example, at Amazon, when you are viewing the page for a particular item, you also are presented information on _other items you might be interested in_ and on _what other items people buy instead_. The first case is an example of item-based collaborative filtering, where the results of other items people have purchased together with this item are identified. The second is an example of user-based collaborative filtering, where the results from other, similar users, are used to identify items that might be of interest. Sometimes, however, the [results of this analysis can be problematic][tpg], and one should always be considerate of the impact predictions might have on the end user (this is clearly an area where _big brother_ impacts everyone).\n",
    "\n",
    "In this notebook, we explore a sample data set, movie reviews, that can be used to learn more about recommender systems. Since Python does not (yet) have a standard implementation of recommender systems, we instead develop a single, user-based collaborative filtering example that uses the movie review data to make recommendations for new movies.\n",
    "\n",
    "-----\n",
    "[rs]: https://en.wikipedia.org/wiki/Recommender_system\n",
    "[wap]: https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    "[tpg]: http://www.workplaceethicsadvice.com/2012/02/target-sends-coupons-to-pregnant-girl-and-unawares-dad-explode.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "[XXX](#XXX)\n",
    "- [YYY](#YYY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Movie Lens Data\n",
    "\n",
    "To begin exploring recommender systems, we will explore the [Movie\n",
    "Lens][ml] data. One problem with recommender systems is that they can\n",
    "use significant resources to make predictions since a large quantity of\n",
    "data is required to make the best predictions. Originally, these\n",
    "algorithms were developed to work with relational database systems,\n",
    "working directly within the database engine for maximal performance. As\n",
    "a result, in this Notebook, we will restrict our analysis to the small\n",
    "Movie Lens data set.\n",
    "\n",
    "We can grab the latest version of the small Movie Lens data set and use\n",
    "within this notebook, however, for this class we have already pulled\n",
    "this data and placed it in the indicated location. Note, to grab the\n",
    "data, you can execute the following Unix commands, either in a code cell\n",
    "or at the Unix command prompt.\n",
    "\n",
    "```bash\n",
    "\n",
    "wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "\n",
    "unzip ml-latest-small.zip\n",
    "```\n",
    "\n",
    "This data set includes a summary document in a file called `README.txt`.\n",
    "This document contains a summary of this data, which is included\n",
    "verbatim below. You can read the entire file either at the command line\n",
    "or via `%load ml-latest-small/README.txt` in a code cell.\n",
    "\n",
    "> Summary\n",
    "> =======\n",
    "\n",
    "> This dataset (ml-latest-small) describes 5-star rating and free-text\n",
    "> tagging activity from [MovieLens](http://movielens.org), a movie\n",
    "> recommendation service. It contains 105339 ratings and 6138 tag\n",
    "> applications across 10329 movies. These data were created by 668 users\n",
    "> between April 03, 1996 and January 09, 2016. This dataset was generated\n",
    "> on January 11, 2016.\n",
    "\n",
    "> Users were selected at random for inclusion. All selected users had\n",
    "> rated at least 20 movies. No demographic information is included. Each\n",
    "> user is represented by an id, and no other information is provided.\n",
    "\n",
    "> The data are contained in four files, `links.csv`, `movies.csv`,\n",
    "> `ratings.csv` and `tags.csv`. More details about the contents and use of\n",
    "> all these files follows.\n",
    "\n",
    "> This is a *development* dataset. As such, it may change over time and is\n",
    "> not an appropriate dataset for shared research results. See available\n",
    "> *benchmark* datasets if that is your intent.\n",
    "\n",
    "> This and other GroupLens data sets are publicly available for download\n",
    "> at <http://grouplens.org/datasets/>.\n",
    "\n",
    "-----\n",
    "\n",
    "In the following code cells, we define the location of this file, and\n",
    "use Unix commands to display the contents of the small Movie Lens data\n",
    "and the number of ratings included.\n",
    "\n",
    "-----\n",
    "[ml]: http://grouplens.org/datasets/movielens/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name of the directory holding the Small MovieLens data\n",
    "data_dir = '/home/data_scientist/data/ml-latest-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access /home/data_scientist/data/ml-latest-small: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc: /home/data_scientist/data/ml-latest-small/ratings.csv: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l $data_dir/ratings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Before developing our recommendation system, we first explore the Movie\n",
    "Lens data. To simplify this task, we will read the ratings data and the\n",
    "movies data into two separate DataFrames. We then display several rows\n",
    "from each DataFrame, as well as compute and display some basic\n",
    "statistics.\n",
    "\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/home/data_scientist/data/ml-latest-small/ratings.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-37aea57e0cdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmovies_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'movies.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/home/data_scientist/data/ml-latest-small/ratings.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "ratings_file = os.path.join(data_dir, 'ratings.csv')\n",
    "movies_file = os.path.join(data_dir, 'movies.csv')\n",
    "\n",
    "ratings = pd.read_csv(ratings_file)\n",
    "movies = pd.read_csv(movies_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for the ratings data set\n",
    "ratings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for the movies data set\n",
    "movies.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique movies in the ratings data file\n",
    "\n",
    "len(pd.unique(ratings['movieId'].ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "As the previous cells indicated, the `ratings` and `movies` DataFrames\n",
    "have a common column, `movieId`, which we can use to join these\n",
    "DataFrames together. This allows us to better understand these data,\n",
    "since we can see the name of the movie being rated (as opposed to just a\n",
    "`movieId`. In the next few code cells, we join these two DataFrames,\n",
    "display a few rows from the new DataFrame, and use Pandas functionality\n",
    "to display the most popular (by number of reviews) movies, as well as\n",
    "the movies that have the highest average ratings. \n",
    "\n",
    "The last task employs several useful tools. First, we perform a\n",
    "`groupby` operation where we employ two operations: mean value and size\n",
    "of sample. This creates a new data structure that has a special column\n",
    "that contains the number of reviews (from the size function) and a\n",
    "special column that contains the average rating (from the mean\n",
    "function). We can display the most popular (in terms of average rating),\n",
    "by first restricting the data structure to only those movies that have\n",
    "been rated twenty or more times, and display the first few rows of the\n",
    "sorted data in descending order.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge two dataframes, the common column is selected aotumatically\n",
    "mv_lens = pd.merge(movies, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_lens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the most commonly rated movies\n",
    "mv_lens.title.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new Data structure that holds the movie, number of ratings, and the average rating.\n",
    "mv_stats = mv_lens.groupby('title').agg({'rating': [np.size, np.mean]})\n",
    "\n",
    "# Number of ratings to consider top movie\n",
    "rating_count = 20\n",
    "\n",
    "# Display most popular movies.\n",
    "top_movies = mv_stats['rating']['size'] >= rating_count\n",
    "mv_stats[top_movies].sort_values(by=('rating', 'mean'), ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### User Based Collaborative Filtering\n",
    "\n",
    "We can now turn our attention to developing a collaborative filter to\n",
    "make recommendations. The basic idea we will employ is to find the user\n",
    "who is most _similar_ to the current user, and use the ratings from this\n",
    "one similar user to make recommendations. To do this, we need two\n",
    "things. First, we need a matrix that contains the movie ratings, indexed\n",
    "by the user id and the movie id. This allows us to find movies to\n",
    "recommend, given a particular user. Second, we need a definition of\n",
    "similarity, which implies a [distance measurement][scdm] (more similar\n",
    "things should be close, while different things should be far apart).\n",
    "There are a number of different distance measures that are employed,\n",
    "including [Euclidean distance][wed], [Manhattan distance][wmd], and\n",
    "[cosine similarity][wcs].\n",
    "\n",
    "In this Notebook, we will employ cosine similarity. However, we will not\n",
    "use the implementation in the scipy library since it is not exactly\n",
    "appropriate for what we need. The cosine similarity treats two sets of\n",
    "data as vectors. For example, we can make a vector for each user where\n",
    "each element in the array corresponds to a rating for a specific movie.\n",
    "obviously, for a rating system with many movies, these vectors are\n",
    "extremely long (in our case, there are $10, 325$ elements). The cosine\n",
    "similarity is calculated by multiplying these two vectors and dividing\n",
    "by their length. Thus, this measurement will be scaled to lie between\n",
    "$-1$ and $1$. If the value is one, the vectors are identical. If the\n",
    "value is minus one, the vectors are exactly opposite of each other. And\n",
    "if the value is zero, the vectors are perpendicular to each other. As a\n",
    "result, the cosine similarity is measuring the angle between the two\n",
    "vectors.\n",
    "\n",
    "To get started, we first construct a pivot table to reference the movie\n",
    "ratings from each user.\n",
    "\n",
    "-----\n",
    "[scdm]: http://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "\n",
    "[wed]: https://en.wikipedia.org/wiki/Euclidean_distance\n",
    "[wmd]: https://en.wikipedia.org/wiki/Taxicab_geometry\n",
    "[wcs]: https://en.wikipedia.org/wiki/Cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pivot table containing ratings indexed by user id and movie id\n",
    "tmp_df = ratings.pivot(index='userId', columns='movieId', values='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The values in this pivot table hold the actual ratings. For simplicity,\n",
    "we can restrict our analysis to only _favorable_ ratings, which, since\n",
    "the movies are rated on a five-star system, we take to mean ratings\n",
    "greater than three. We convert the pivot table to hold one for favorable\n",
    "ratings and zero for unfavorable ratings and convert the result to a\n",
    "numpy matrix. The shape of the matrix indicates we have $668$ unique\n",
    "users, who have each rated one or more of $10,325$ movies.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_data = tmp_df.applymap(lambda x: 1 if x > 3 else 0).as_matrix()\n",
    "print(the_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "This matrix is quite large, if we were processing the full movie data\n",
    "set, it would be prohibitively expensive. Fortunately, there is a simple\n",
    "fact that we can leverage. In general, we will not want to include items\n",
    "that haven't been rated enough times. This concept is sometimes called\n",
    "the _support_. Without sufficient ratings, we can't make accurate\n",
    "predictions as the power of this algorithm comes from leveraging data\n",
    "collected from multiple people who (ideally at least) have rated many\n",
    "things.\n",
    "\n",
    "As a result, we will generate a new pivot table, by first grouping the\n",
    "movies together, sorting into descending order, and selecting only those\n",
    "movies that have been reviewed by multiple people (in this case we will\n",
    "select only movies that have been reviewed at least `rating_count` or\n",
    "more times. By default in this Notebook, this variable is twenty, which\n",
    "means only those movies rated at least twenty times will be included in\n",
    "our analysis. Next, we create our pivot table as before, and change the\n",
    "ratings into either zero (if rated three or less) or one (if rated four\n",
    "or five). The shape of the matrix is now considerably smaller, with only\n",
    "$152$ unique users and $873$ unique movies.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvrs = ratings.groupby('movieId').size().sort_values(ascending=False)\n",
    "tmp_ratings = ratings.ix[mvrs[mvrs > rating_count].index].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = tmp_ratings.pivot(index='userId', columns='movieId', values='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_data = tmp_df.applymap(lambda x: 1 if x > 3 else 0).as_matrix()\n",
    "print(the_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "We now create our function to compute the cosine similarity between our\n",
    "two vectors. We use simple numpy commands, and subsequently demonstrate\n",
    "it being used on sample vectors.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the Cosine Similarity function\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    return(np.dot(u, v)/np.sqrt((np.dot(u, u) * np.dot(v, v))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 1, 1, 0, 0])\n",
    "b = np.array([0, 0, 0, 1, 1])\n",
    "c = np.array([0, 1, 0, 1, 1])\n",
    "\n",
    "print('cosine similarity(a, b) = {0:4.3f}'.format(cosine_similarity(a, b)))\n",
    "print('cosine similarity(a, c) = {0:4.3f}'.format(cosine_similarity(a, c)))\n",
    "print('cosine similarity(b, c) = {0:4.3f}'.format(cosine_similarity(b, c)))\n",
    "\n",
    "print('cosine similarity(a, a) = {0:4.3f}'.format(cosine_similarity(a, a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Single User\n",
    "\n",
    "We now develop the algorithm to make a recommendation for a single user.\n",
    "To do this, we first create a _fake_ user, by selecting several movies\n",
    "as favorable (effectively we simply make a new user vector). Given this\n",
    "new vector, we compute the cosine similarity between this new user and\n",
    "all users in our reduced data user-movie matrix. We identify the user\n",
    "who is most similar by selecting the row in the user-movie matrix with\n",
    "the highest cosine similarity, extract the movies favorably rated by\n",
    "this particular user, remove any that have already been rated by our\n",
    "_fake_ user, and display the results.\n",
    "\n",
    "To simplify the identification of the correct movie title, we add a new\n",
    "column to the DataFrame that holds the joined rating and movie data to\n",
    "map between movieId in our user-movie matrix and the movieID used in the\n",
    "original data.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The user-movie matrix\n",
    "\n",
    "x = the_data\n",
    "\n",
    "# Make a fake user (with movie ratings that will gaurantee a match)\n",
    "y = np.zeros(the_data.shape[1], dtype=np.int32)\n",
    "y[6] = 1 ; y[10] = 1; y[15] = 1; y[64] = 1; y[136] = 1\n",
    "y[180] = 1; y[230] = 1; y[339] = 1; y[622] = 1; y[703] = 1\n",
    "\n",
    "# Add a special index column to map the row in the x matrix to the userIds\n",
    "tmp_df.tmp_idx = np.array(range(x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity, find maximum value\n",
    "sims = np.apply_along_axis(cosine_similarity, 1, x, y)\n",
    "mx = np.nanmax(sims)\n",
    "\n",
    "# Find the best matching user\n",
    "usr_idx = np.where(sims==mx)[0][0]\n",
    "\n",
    "# Print the first thirty reviews of test user and matched user.\n",
    "print(y[:30])\n",
    "print(x[usr_idx, :30])\n",
    "\n",
    "print('\\nCosine Similarity(y, x[{0:d}]) = {1:4.3f}' \\\n",
    "      .format(usr_idx, cosine_similarity(y, x[usr_idx])), end='\\n\\n')\n",
    "\n",
    "# Now we subtract the vectors\n",
    "# (any negative value is a movie to recommend)\n",
    "mov_vec = y - x[usr_idx]\n",
    "\n",
    "# We want a mask aray, so we zero out any recommended movie.\n",
    "mov_vec[mov_vec >= 0] = 1\n",
    "mov_vec[mov_vec < 0] = 0\n",
    "\n",
    "print(mov_vec[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the number of movies we will recommend.\n",
    "print('\\n{0} Movie Recommendations for User = {1}' \\\n",
    "      .format(mov_vec[mov_vec == 0].shape[0], \n",
    "              tmp_df[tmp_df.tmp_idx == usr_idx].index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns (movieIds) for the current user\n",
    "mov_ids = tmp_df[tmp_df.tmp_idx == usr_idx].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make a masked array to find movies to recommend\n",
    "# values are the movie ids, mask is the movies the most\n",
    "# similar user liked.\n",
    "\n",
    "ma_mov_idx = ma.array(mov_ids, mask = mov_vec)\n",
    "mov_idx = ma_mov_idx[~ma_mov_idx.mask]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make a DataFrame of the moves of interest and display\n",
    "\n",
    "mv_df = movies.ix[movies.movieId.isin(mov_idx)].dropna()\n",
    "\n",
    "print(60*'-')\n",
    "\n",
    "for movie in mv_df.title.values:\n",
    "    print(movie)\n",
    "\n",
    "print(60*'-', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used cosine similarity to find the user most\n",
    "similar to a test (or fake) user in order to make recommendations. Now\n",
    "that you have run the Notebook, go back and make the following changes\n",
    "to see how the results change.\n",
    "\n",
    "1. Make a new fake user, ensure that there are movies in common with\n",
    "more than one real user. What movies are now recommended? Do they make\n",
    "sense?\n",
    "2. Try changing the `ratings_count` variable higher and lower. How does\n",
    "this change the results of the recommendation? Note this affects both\n",
    "the construction of the user-movie DataFrame and the final matrix.\n",
    "3. The current algorithm uses the only favorable ratings, but that isn't\n",
    "necessary. Try mapping the ratings to the space $-1, 1$ and repeating\n",
    "the analysis. In this mapping $-1$ corresponds to an unfavorable rating\n",
    "of one. How do the results change?\n",
    "\n",
    "Finally, the current algorithm finds the _best matching_ user and uses\n",
    "that user's ratings to make a recommendation. In reality, we would want\n",
    "to use $n$ best matching users to make recommendation. Change the\n",
    "algorithm to capture the five most similar users, and display the\n",
    "aggregated results.\n",
    "\n",
    "-----\n",
    "\n",
    "### Multiple User Recommendation\n",
    "\n",
    "The previous example provided recommendations for a single user. We can\n",
    "extend this example to make recommendations for multiple users. In this\n",
    "case, however, we still use the same basic algorithm. First, we divide\n",
    "our data into two samples: training and testing. We then iterate through\n",
    "our testing set to find the best matching user to compute\n",
    "recommendations for the current testing sample user. Finally, the\n",
    "recommendations are displayed for each user in the test sample.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x, y = the_data, range(the_data.shape[0])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state=42)\n",
    "\n",
    "# Add an index into the user-movie DataFrame for the movies that are in the\n",
    "# user-movie matrix.\n",
    "tmp_df.tmp_idx = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each user in test set.\n",
    "for idx, user in enumerate(x_test):\n",
    "    \n",
    "    # Compute similarity, find maximum value\n",
    "    sims = np.apply_along_axis(cosine_similarity, 1, x_train, user)\n",
    "    mx = np.nanmax(sims)\n",
    "    \n",
    "    # If maximum value is a real value    \n",
    "    if mx > 0:\n",
    "        \n",
    "        # Find the index in the similarity matrix with maximum value\n",
    "        train_idx = np.where(sims==mx)[0][0]\n",
    "        \n",
    "        # Now we subtract the vectors \n",
    "        # (any negative value is a movie to recommend)\n",
    "        mov_vec = user - x_train[train_idx]\n",
    "        \n",
    "        # We make a mask aray, so we zero out any recommended movie.\n",
    "        mov_vec[mov_vec >= 0] = 1\n",
    "        mov_vec[mov_vec < 0] = 0\n",
    "        \n",
    "        # We use the fact that y_train has the indices into the original\n",
    "        # temporary data frame\n",
    "\n",
    "        user_idx = tmp_df[tmp_df.tmp_idx == y_train[train_idx]]\n",
    "\n",
    "        # State how many movies are being recommend for this user id\n",
    "        print('{0} Movie Recommendations for User = {1}' \\\n",
    "              .format(mov_vec[mov_vec == 0].shape[0], \\\n",
    "                      tmp_df[tmp_df.tmp_idx == y_test[idx]].index[0]))\n",
    "        \n",
    "        print(60*'-')\n",
    "        # Now make a masked array to find movies to recommend\n",
    "        # values are the movie ids, mask is the movies the most\n",
    "        # similar user liked.\n",
    "        ma_mov_idx = ma.array(user_idx.columns, mask = mov_vec)\n",
    "        mov_idx = ma_mov_idx[~ma_mov_idx.mask]\n",
    "        \n",
    "        # Now make a DataFrame of the moves of interest and display\n",
    "        mv_df = movies.ix[movies.movieId.isin(mov_idx)].dropna()\n",
    "        for movie in mv_df.title.values:\n",
    "            print(movie)\n",
    "            \n",
    "        print(60*'-', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cell, we computed the cosine similarity between test and training data to make recommendations.  Now that you have run the Notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Right now, the algorithm uses **only** the best matching user. Change the algorithm to use the five best matching users to develop a more complete set of movie ratings.\n",
    "\n",
    "2. In some cases, the best matching user does not exist. Change the `rating_count` variable so that all movies are included. How does this change the performance and results of the algorithm?\n",
    "\n",
    "This example has been a user-based collaborative filtering. If we transpose our matrix, however, we have a movie-user matrix, which can be used to make item-based collaborative filtering recommendations. In this case, you will find users who have rated the current movie and use their other ratings to find movies to recommend. Implement this new algorithm by following this description  and the previous code.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. A nice introduction to [recommendation systems][1] from IBM\n",
    "2. Wikipedia article on [Recommender system][wr]\n",
    "3. Wikipedia article on [Apriori algorithm][wa]\n",
    "4. Wikipedia article on [Cosine similarity][wc]\n",
    "5. An article on [recommender systems][2] from the Encyclopedia of Machine Learning\n",
    "6. Article on [association rules][3]\n",
    "-----\n",
    "\n",
    "[1]: https://www.ibm.com/developerworks/library/os-recommender1/index.html\n",
    "[wr]: https://en.wikipedia.org/wiki/Recommender_system\n",
    "[wa]: https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    "[wc]: https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "[2]: http://www.prem-melville.com/publications/recommender-systems-eml2010.pdf\n",
    "[3]: http://aimotion.blogspot.com/2013/01/machine-learning-and-data-mining.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
