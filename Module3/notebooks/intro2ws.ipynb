{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we have looked at different data formats, including the CSV, JSON and XML text-based formats. In this notebook, we explore how to actually pull data of interest out of  semi-structured and structured text data. To do this, we begin by reviewing the concept of parsing, where we use the structure of a document to extract contextual information. Next, we move on to parsing structured documents, for which we use the parsing tool [BeautifulSoup][bs]. This library provides an elegant and simple method to parse and access XML formatted data, which includes HTML and SVG documents. BeautifulSoup was actually designed to simplify the task of scraping data from websites; thus we can use it to parse any XML formatted data. \n",
    "\n",
    "-----\n",
    "[bs]: http://www.crummy.com/software/BeautifulSoup/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "[Sample Document](#Sample-Document)\n",
    "\n",
    "[Document Object Model](#Document-Object-Model)\n",
    "\n",
    "[Parsing Documents](#Parsing-Documents)\n",
    "\n",
    "[Using Regular Expressions](#Using-Regular-Expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Sample Document\n",
    "\n",
    "To demonstrate parsing, we need a sample, XML compliant document. For this purpose, we create an HTML document and assign it to the `html` variable in the following Code cell. This document has a defined _doctype_, follows standard HTMl rules including the use of a parent _html_ element, both _head_ and _body_ elements, as well as several other HTML elements including a paragraph element, a header element, an unordered list element, a table element, and a footer element. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple HTML document to demonstrate DOM processing\n",
    "\n",
    "html = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head id='hid' class='hclass'>\n",
    "<title> Test, this is only a test ... </title>\n",
    "</head>\n",
    "<body id='bid' class='bclass'>\n",
    "<header> \n",
    "This is text in the header.\n",
    "</header>\n",
    "\n",
    "<h2 color='mycolor'>This is a Header Level 2</h2>\n",
    "\n",
    "<p align='myalign'>Here is some text in a paragraph.</p>\n",
    "\n",
    "<p> Here is a list </p>\n",
    "<ul id='ulid'>\n",
    "<li> List Item #1 </li>\n",
    "<li> List Item #2 </li>\n",
    "</ul>\n",
    "\n",
    "<p type='caption'> Here is a table </p>\n",
    "<table id='tid'>\n",
    "<tr>\n",
    "<th> Column #1 </th>\n",
    "<th> Column #2 </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> A value </td>\n",
    "<td> Another Value </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<p> Some concluding text </p>\n",
    "\n",
    "<footer>\n",
    "<hr />\n",
    "This is a text in the footer.\n",
    "</footer>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "In the following Code cell, we display the HTML document inline, showing the different document components mentioned earlier. The second Code cell below, writes the document to a file to simplify subsequent parsing.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head id='hid' class='hclass'>\n",
       "<title> Test, this is only a test ... </title>\n",
       "</head>\n",
       "<body id='bid' class='bclass'>\n",
       "<header> \n",
       "This is text in the header.\n",
       "</header>\n",
       "\n",
       "<h2 color='mycolor'>This is a Header Level 2</h2>\n",
       "\n",
       "<p align='myalign'>Here is some text in a paragraph.</p>\n",
       "\n",
       "<p> Here is a list </p>\n",
       "<ul id='ulid'>\n",
       "<li> List Item #1 </li>\n",
       "<li> List Item #2 </li>\n",
       "</ul>\n",
       "\n",
       "<p type='caption'> Here is a table </p>\n",
       "<table id='tid'>\n",
       "<tr>\n",
       "<th> Column #1 </th>\n",
       "<th> Column #2 </th>\n",
       "</tr>\n",
       "<tr>\n",
       "<td> A value </td>\n",
       "<td> Another Value </td>\n",
       "</tr>\n",
       "</table>\n",
       "\n",
       "<p> Some concluding text </p>\n",
       "\n",
       "<footer>\n",
       "<hr />\n",
       "This is a text in the footer.\n",
       "</footer>\n",
       "\n",
       "</body>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "display_html(html, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Now that we have our sample document, we save it the local filesystem to simplify subsequent parsing. First, we define our local data directory, before creating a file for the document.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we find our HOME directory\n",
    "home_dir = !echo $HOME\n",
    "\n",
    "# Define data directory\n",
    "data_dir = home_dir[0] +'/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now save the HTML string\n",
    "with open(data_dir + 'test.html', 'w') as fout:\n",
    "    fout.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Document Object Model\n",
    "\n",
    "There are at least two techniques used to parse a structured file like an XML document. The first approach is known as [Simple API for XML][sax] (or SAX), which is an event driven parser that reads and processes each part of an XML document sequentially. The second approach is the [Document Object Model][dom] (or DOM), which reads and parses the entire document. While the SAX approach can be fast and uses a smaller memory footprint, the DOM approach can be more easily used to extract all or most of the information contained in an XML document. \n",
    "\n",
    "To demonstrate using a DOM, we can process our newly minted [HTML file](test.html), which is rendered rather simply as shown in the following figure:\n",
    "\n",
    "![HTML Page view](images/html-view.png)\n",
    "\n",
    "This HTML document, which is a valid XML document, demonstrates both hierarchical elements, as well as element attributes and values. This can be seen more easily by examining the document object model (or DOM) representation of this document, which is shown in the following figure:\n",
    "\n",
    "![HTML DOM view](images/html-dom.png) \n",
    "\n",
    "This figure is actually a screenshot from the Safari Web Browser _Developer Source View_, other browsers provide similar functionality (although you may  need to install an add-on package).  This representation of the DOM very clearly illustrates the hierarchical nature of the document. At the highest level we have the `html` element, inside of which are two separate elements: `body` and `head`. \n",
    "\n",
    "![HTML DOM view](images/dom-tree.png) \n",
    "\n",
    "Looking at the document tree more closely, we see that the `head` element has an associated `id` and `class` attributes as well as a child element called `title`, which has a value of  `Test, this is only a test ...`. The `body` element has a number of children elements, including the `header`, `h2`, `p`, `ul`, `table`, and `footer` elements. Some of these elements have both child elements, values, and possibly their own attributes. The relationship between the DOM element and the HTML view can be seen in the following two figures, where the `ul` element is highlighted in the DOM model, \n",
    "\n",
    "![HTML DOM element](images/dom-element.png) \n",
    "\n",
    "and the corresponding element is highlighted in the HTML view.\n",
    "\n",
    "![HTML html element](images/html-element.png) \n",
    "\n",
    "-----\n",
    "[sax]: https://en.wikipedia.org/wiki/Simple_API_for_XML\n",
    "[dom]: https://en.wikipedia.org/wiki/Document_Object_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Parsing Documents\n",
    "\n",
    "To parse an XML document, like our example HTML document, we can use the Python [Beautiful Soup][bs] library. This library uses an XML/HTML parser to build a DOM tree, and Beautiful Soup then provides traversal methods to access and modify the DOM for a specific document. BeautifulSoup has been extremely popular for the ease with which it allows web scraping, for example, you can pull data out of an HTML table. But it is more powerful than this, as it allows you to easily parse and manipulate any XML document.\n",
    "\n",
    "To use Beautiful Soup, we first need to import the library, and then create a BeautifulSoup object that provides access to the parsed data. Document elements, like `body` or `table` are directly accessed from the parsed tree; and element attributes or data can be easily extracted, deleted, or replaced. If required, new data can also be added to an existing document, allowing for the dynamic creation of a new document. These capabilities are demonstrated in the following code cells.\n",
    "\n",
    "-----\n",
    "[bs]: http://www.crummy.com/software/BeautifulSoup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head class=\"hclass\" id=\"hid\">\n",
      "  <title>\n",
      "   Test, this is only a test ...\n",
      "  </title>\n"
     ]
    }
   ],
   "source": [
    "# Parse our HTML document\n",
    "\n",
    "# We use BeautifulSoup version 4\n",
    "from bs4 import BeautifulSoup\n",
    "  \n",
    "# load our doucment, and specify parser\n",
    "soup = BeautifulSoup(open(data_dir + 'test.html'), 'lxml')\n",
    "\n",
    "# Now lets print out the start of the HTMl file\n",
    "print(soup.prettify()[:108])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "To extract an element, we simply use the element's name as an attribute. Thus, to extract the title of the HTML document, we use `soup.title`. These elements have several special attributes, including `name` to extract the name of the element and the `string` attribute to extract the data enclosed between the opening and closing element tags. We can also traverse the DOM tree by requesting the parent element by using the `parent` attribute on an element.\n",
    "\n",
    "The following Code cell demonstrate these concepts by extracting the _title_ element, the data within the _title_ element, and the name of the _title_ element's parent element.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title element:=  <title> Test, this is only a test ... </title>\n",
      "title value:  Test, this is only a test ... \n",
      "title parent element:  head\n"
     ]
    }
   ],
   "source": [
    "# We can access document elements directly\n",
    "print('title element:= ', soup.title)\n",
    "print('title value:', soup.title.string)\n",
    "\n",
    "# We can access parent data\n",
    "print('title parent element: ', soup.title.parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can also access element attributes by using a dictionary-style access method. For example, the following Code cell extracts the `class` attribute from the _body_ element.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body class attribute:  ['bclass']\n"
     ]
    }
   ],
   "source": [
    "# We can directly access element attributes\n",
    "print('body class attribute: ', soup.body['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Accessing an element directly provides the entire content of the element, even child elements. This is demonstrated in the following Code cell, where the unordered list (_ul_) element is accessed, providing the entire list contents.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ul id=\"ulid\">\n",
      "<li> List Item #1 </li>\n",
      "<li> List Item #2 </li>\n",
      "</ul>\n"
     ]
    }
   ],
   "source": [
    "# We can access an entire element's content\n",
    "print(soup.ul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can also search for all elements in a document, and iterate through the search results using a Python loop. This functionality is provided by the `find_all` function, which takes an element and returns a result set containing all matches. This set can be iterated over to provide access to each matching element, as shown in in the next Code cell that finds all paragraph elements.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p align=\"myalign\">Here is some text in a paragraph.</p>\n",
      "<p> Here is a list </p>\n",
      "<p type=\"caption\"> Here is a table </p>\n",
      "<p> Some concluding text </p>\n"
     ]
    }
   ],
   "source": [
    "# We can find all occurances of a particular element\n",
    "\n",
    "for el in soup.find_all('p'):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "By accessing the elements, we can also change their values. For example, we can change the title of the document or the attributes of an element. Changing content simply requires assigning the new value to the element or the attribute. These concepts are demonstrated in the next Code cell where we modify the content of our HTML document\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New title = <title>This is a new title!</title>\n",
      "\n",
      "Body class attribute = newClass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also change data in the document\n",
    "soup.title.string = 'This is a new title!'\n",
    "print(f'New title = {soup.title}\\n')\n",
    "\n",
    "# Change attribute and display\n",
    "soup.body['class'] = 'newClass'\n",
    "print(f'Body class attribute = {soup.body[\"class\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can remove elements from the document by using the [`extract`][bse] function. Applying this function directly to an element (or tag) removes the tag (and its children) from the XML document. We demonstrate this in the next Code cell where we remove the entire table element from the document.\n",
    "\n",
    "-----\n",
    "[bse]: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# We can delete elements, the display is \n",
    "# None since the element is gone\n",
    "myTable = soup.table.extract()\n",
    "print(soup.table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can also select elements based on a Cascading Style Sheet (CSS), which follows an attribute style access. The following example demonstrates selecting a paragraph element with the `type` attribute, which can be used by a CSS document to apply a styling to the element.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p type=\"caption\"> Here is a table </p>]\n"
     ]
    }
   ],
   "source": [
    "# We can select elements based on CSS Selectors\n",
    "target = soup.select('p[type]')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can also insert new content into a document. The simplest approach is to use the `insert_before` or `insert_after` functions, which insert the new element before or after, respectively, the indicated element. In the following example, we insert the table we removed earlier, and place it right after the paragraph element we selected int he previous Code cell.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table id=\"tid\">\n",
      "<tr>\n",
      "<th> Column #1 </th>\n",
      "<th> Column #2 </th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td> A value </td>\n",
      "<td> Another Value </td>\n",
      "</tr>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "# We need to pull out the first element in the list to get tag\n",
    "# Now we can insert our table back into the DOM\n",
    "\n",
    "target[0].insert_after(myTable)\n",
    "print(soup.table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can also create entirely new content. To do this, we need to create the new tag, and add it to an existing tag. The [`insert`][bsi] function takes a first argument that specifies the insert position within the existing element. For example, `1` means at the start of the existing element. These concepts are demonstrated in the next Code cell where we modify the content of our HTML document\n",
    "\n",
    "-----\n",
    "\n",
    "[bsi]: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New header element = <h3 id=\"h3id\">A New Header</h3>\n",
      "\n",
      "New title element = <body_title>A body title</body_title>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also insert entirely new elements.\n",
    "\n",
    "# First we create a new element (tag), with an attribute\n",
    "tag = soup.new_tag('h3', id='h3id')\n",
    "tag.string = 'A New Header'\n",
    "\n",
    "# Now we can append (in this case we append to the end of the body element)\n",
    "soup.body.append(tag)\n",
    "print(f'New header element = {soup.h3}\\n')\n",
    "\n",
    "# Now create a new tag\n",
    "nt = soup.new_tag('body_title')\n",
    "nt.string = \"A body title\"\n",
    "\n",
    "# Insert at start of the body element\n",
    "soup.body.insert(1, nt)\n",
    "print(f'New title element = {soup.body.body_title}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Using Regular Expressions\n",
    "\n",
    "While Beautiful Soup provides a great deal of power and simplicity in DOM parsing and element retrieval, the full power of parsing a document requires the use of regular expressions. We introduced regular expressions in a previous lesson; for completeness, however, we briefly review their use in Python.\n",
    "\n",
    "Regular expressions, or RE or regexes, are expressions that can be used to match one or more occurrences of a particular pattern. Regular expressions are not unique to Python, they are used in many programming languages and many Unix command line tools like `sed`, `grep`, or `awk`. [Regular expressions][re] are used in Python through the `re` module. Given a regular expression, the first task in Python is to compile the RE, which is done by using the `compile` method in the `re` module. \n",
    "\n",
    "We demonstrate this approach in the following Code cell, where we now parse a much larger document, the airport XML document created in an earlier lesson. Below, we use a regular expression to find and display the element containing `CMI` to display our local airport.\n",
    "\n",
    "-----\n",
    "[re]: https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<airport name=\"University of Illinois-Willard\">\n",
      "<iata>CMI</iata>\n",
      "<city>Champaign/Urbana</city>\n",
      "<state>IL</state>\n",
      "<country>USA</country>\n",
      "<latitude>40.03925</latitude>\n",
      "<longitude>-88.27805556</longitude>\n",
      "</airport>\n"
     ]
    }
   ],
   "source": [
    "# We need the re module\n",
    "import re \n",
    "\n",
    "# Open and parse our XML document\n",
    "soup = BeautifulSoup(open(data_dir + 'data.xml'), 'lxml')\n",
    "\n",
    "# Findelements containing the CMI string\n",
    "for el in soup.find_all(text=re.compile('CMI')):\n",
    "\n",
    "    # To get the entire airport element, we need to go \n",
    "    # up two levels in the DOM tree.\n",
    "    print(el.parent.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "Earlier in this notebook, we used the BeautifulSoup module, the libXML parser, and regular expressions to extract information from web pages. Now that you have run the cells in this notebook, go back to the relevant cells and make these changes. Be sure to understand how your changes impact the file input and output process.\n",
    "\n",
    "3. Add an ordered HTML list containing at least five elements (i.e., use an `<ol>` element, with five child `<li>` elements to the original HTML document. Use the BeautifulSoup library to extract and display the five items.\n",
    "4. Change all words in the HTML document to be upper-case.\n",
    "56. Use a regular expression to find and display all airports in the _data.xml_ document located within the state of Wyoming.\n",
    "\n",
    "As a challenge problem:\n",
    "\n",
    "1. Save several webpages (perhaps by using wget), and modify the BeautifulSoup code example to parse out and display the page title, any Javascript code libraries, and any css style file references.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "2. [BeautifulSoup][2] tutorial.\n",
    "3. The [scrapy][3] web parsing tool\n",
    "23. An older, but easy to follow blog article on using [BeautifulSoup][4] to parse a webpage.\n",
    "43. A tutorial notebook on [web scraping][43] with Python\n",
    "\n",
    "-----\n",
    "\n",
    "[2]: http://programminghistorian.org/lessons/intro-to-beautiful-soup\n",
    "[3]: http://scrapy.org\n",
    "[4]: https://programminghistorian.org/lessons/intro-to-beautiful-soup\n",
    "[43]: http://nbviewer.jupyter.org/url/www.unc.edu/%7Encaren/Lax-1.ipynb.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
