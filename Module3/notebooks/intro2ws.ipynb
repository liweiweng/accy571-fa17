{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we have looked at different formats, including the CSV, JSON and XML formats. In this notebook, we explore how to actually pull data of interest out of  semi-structured and structured text data. To do this, we begin by reviewing the concept of parsing, where we use the structure of a document to extract contextual information. Next, we move on to parsing structured documents, for which we use the parsing tool [BeautifulSoup][bs], which provides an elegant and simple method to parse and access XML formatted data. BeautifulSoup was actually designed to simplify the task of scraping data from websites, and thus we can use it to parse any XML formatted data including HTML or SVG. \n",
    "\n",
    "-----\n",
    "[bs]: http://www.crummy.com/software/BeautifulSoup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write out a simple HTML file to demonstrate DOM processing\n",
    "\n",
    "html = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head id='hid' class='hclass'>\n",
    "<title> Test, this is only a test ... </title>\n",
    "</head>\n",
    "<body id='bid' class='bclass'>\n",
    "<header> \n",
    "This is text in the header.\n",
    "</header>\n",
    "\n",
    "<h2 color='mycolor'>This is a Header Level 2</h2>\n",
    "\n",
    "<p align='myalign'>Here is some text in a paragraph.</p>\n",
    "\n",
    "<p> Here is a list </p>\n",
    "<ul id='ulid'>\n",
    "<li> List Item #1 </li>\n",
    "<li> List Item #2 </li>\n",
    "</ul>\n",
    "\n",
    "<p type='caption'> Here is a table </p>\n",
    "<table id='tid'>\n",
    "<tr>\n",
    "<th> Column #1 </th>\n",
    "<th> Column #2 </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> A value </td>\n",
    "<td> Another Value </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<p> Some concluding text </p>\n",
    "\n",
    "<footer>\n",
    "<hr />\n",
    "This is a text in the footer.\n",
    "</footer>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Now save the HTML string\n",
    "with open('test.html', 'w') as fout:\n",
    "    fout.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Object Model\n",
    "\n",
    "There are at least two techniques used to parse a structured file like\n",
    "an XML document. The first approach is known as [Simple API for XML][sax] (or\n",
    "SAX), which is an event drieven parser that reads and processes each\n",
    "part of an XML document sequentially. The second approach is the\n",
    "[Document Object Model][dom] (or DOM), which reads and parses the entire\n",
    "document. While the SAX approach can be fast and uses a smaller memory\n",
    "footprint, the DOM approach can be more easily used to extract all or\n",
    "most of the information contained in an XML document. \n",
    "\n",
    "To demonstrate using a DOM, we can process our newly minted [HTML\n",
    "file](test.html), which is rendered rather simply as shown in the\n",
    "following figure:\n",
    "\n",
    "![HTML Page view](images/html-view.png)\n",
    "\n",
    "This HTML document, which is a valid XML document, demonstrates both\n",
    "hierarchical elements, as well as element attributes and values. This\n",
    "can be seen more easily by examining the document object model (or DOM)\n",
    "representation of this document, which is shown in the following figure:\n",
    "\n",
    "![HTML DOM view](images/html-dom.png) \n",
    "\n",
    "This figure is actually a screenshot from the Safari Web Browser\n",
    "_Developer Source View_.  This representation of the DOM very clearly\n",
    "illustrates the hierarchical nature of the document. At the highest\n",
    "level we have the `html` element, inside of which are two separate\n",
    "elements: `body` and `head`. \n",
    "\n",
    "![HTML DOM view](images/dom-tree.png) \n",
    "\n",
    "Looking at the document tree more closely, we see that the `head`\n",
    "element has an associated `id` and `class` atributes as well as a child\n",
    "element called `title`, which has a value of  `Test, this is only a test\n",
    "...`. The `body` element has a number of children elements, including\n",
    "the `header`, `h2`, `p`, `ul`, `table`, and `footer` elements. Some of\n",
    "these elements have both child elements, values, and possibly their own\n",
    "attributes. The relationship between the DOM element and the HTML view\n",
    "can be seen in the following two figures, where the `ul` element is\n",
    "highlighted in the DOM model, \n",
    "\n",
    "![HTML DOM element](images/dom-element.png) \n",
    "\n",
    "and the corresponding element is highlighted in the HTML view.\n",
    "\n",
    "![HTML html element](images/html-element.png) \n",
    "\n",
    "-----\n",
    "[sax]: https://en.wikipedia.org/wiki/Simple_API_for_XML\n",
    "[dom]: https://en.wikipedia.org/wiki/Document_Object_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "\n",
    "To parse an XML document, like our example HTML document, we can use the\n",
    "Python [Beautiful Soup][bs] library. This library uses an XML/HTML\n",
    "parser to build a DOM tree, and Beautiful Soup then provides traversal\n",
    "methods to access and modify the DOM for a specific document. Beautiful\n",
    "Soup has been extremely popular for the ease with which it allows web\n",
    "scraping, for example, you can pull data out of an HTML table. But it is\n",
    "more powerful than this, as it allows you to easily parse and\n",
    "manipulate any XML document as we will see in the [Data\n",
    "Visualization][ds] Notebook.\n",
    "\n",
    "To use Beautiful Soup, we first need to import the library, and then\n",
    "create a BeautifulSoup object that provides access to the parsed data.\n",
    "Document elements, like `body` or `table` are directly accessed from the\n",
    "parsed tree; and element attributes or data can be easily extracted,\n",
    "deleted, or replaced. If required, new data can also be added to an\n",
    "existing document, allowing for the dynamic creation of a new document.\n",
    "These capabilities are demonstrated in the following code cells.\n",
    "\n",
    "-----\n",
    "[bs]: http://www.crummy.com/software/BeautifulSoup/\n",
    "[ds]: dataviz.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head class=\"hclass\" id=\"hid\">\n",
      "  <title>\n",
      "   Test, this is only a test ...\n",
      "  </title>\n"
     ]
    }
   ],
   "source": [
    "# Lets parse our HTML document\n",
    "\n",
    "# We use BeautofulSoup version 4\n",
    "from bs4 import BeautifulSoup\n",
    "  \n",
    "soup = BeautifulSoup(open('test.html'), 'lxml')\n",
    "\n",
    "# Now lets print out the start of the HTMl file\n",
    "print(soup.prettify()[:108])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title element:=  <title> Test, this is only a test ... </title>\n",
      "title value:  Test, this is only a test ... \n",
      "title parent element:  head\n"
     ]
    }
   ],
   "source": [
    "# We can access document elements directly\n",
    "print('title element:= ', soup.title)\n",
    "print('title value:', soup.title.string)\n",
    "\n",
    "# We can access parent data\n",
    "print('title parent element: ', soup.title.parent.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body class attribute:  ['bclass']\n"
     ]
    }
   ],
   "source": [
    "# We can directly access elemnt attributes\n",
    "\n",
    "print('body class attribute: ', soup.body['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ul id=\"ulid\">\n",
      "<li> List Item #1 </li>\n",
      "<li> List Item #2 </li>\n",
      "</ul>\n"
     ]
    }
   ],
   "source": [
    "# We can access an entire element's content\n",
    "print(soup.ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p align=\"myalign\">Here is some text in a paragraph.</p>\n",
      "<p> Here is a list </p>\n",
      "<p type=\"caption\"> Here is a table </p>\n",
      "<p> Some concluding text </p>\n"
     ]
    }
   ],
   "source": [
    "# We can find all occurances of a particular element\n",
    "\n",
    "for el in soup.find_all('p'):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>This is a new title!</title>\n",
      "\n",
      "Body class attribute =  newClass\n"
     ]
    }
   ],
   "source": [
    "# We can also change data in the document\n",
    "\n",
    "soup.title.string = 'This is a new title!'\n",
    "\n",
    "print(soup.title)\n",
    "\n",
    "soup.body['class'] = 'newClass'\n",
    "\n",
    "print(\"\\nBody class attribute = \", soup.body['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# We can delete elements\n",
    "\n",
    "myTable = soup.table.extract()\n",
    "\n",
    "print(soup.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p type=\"caption\"> Here is a table </p>]\n"
     ]
    }
   ],
   "source": [
    "# We can select elements based on CSS Selectors\n",
    "target = soup.select('p[type]')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table id=\"tid\">\n",
      "<tr>\n",
      "<th> Column #1 </th>\n",
      "<th> Column #2 </th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td> A value </td>\n",
      "<td> Another Value </td>\n",
      "</tr>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "# We need to pull out the first element in the list to get tag\n",
    "# Now we can insert our table back into the DOM\n",
    "\n",
    "target[0].insert_after(myTable)\n",
    "print(soup.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3 id=\"h3id\">A New Header</h3>\n"
     ]
    }
   ],
   "source": [
    "# We can also insert entirely new elements.\n",
    "\n",
    "# First we create a new element (tag)\n",
    "tag = soup.new_tag('h3', id='h3id')\n",
    "tag.string = 'A New Header'\n",
    "\n",
    "# Now we can append (in this case we put the new element at the end of the body)\n",
    "\n",
    "soup.body.append(tag)\n",
    "\n",
    "# Show the result\n",
    "print(soup.h3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "While Beautiful Soup provides a great deal of power and simplicity in\n",
    "DOM parsing and element retrieval, the full power of parsing a document\n",
    "requires the use of regular expressions. We covered regular expressions\n",
    "in Week 6; for completeness, however, we briefly review their use in\n",
    "Python.\n",
    "\n",
    "### Regular Expressions\n",
    "\n",
    "Regular expressions, or RE or regexes, are expressions that can be used\n",
    "to match one or more occurrences of a particular pattern. Regular\n",
    "expressions are not unique to Python, they are used in many programming\n",
    "languages and many Unix command line tools like sed, grep, or awk.\n",
    "[Regular expressions][re] are used in Python through the `re` module. To\n",
    "build a regular expression, you need to understand the syntax of the RE\n",
    "language. Once a regular expression is developed, it is compiled and\n",
    "executed by an engine written in C in order to provide fast execution.\n",
    "\n",
    "To begin, most characters in a regular expression simply match\n",
    "themselves, For example `python` would match any occurrence of the six\n",
    "letters `python` either alone or embedded in another word. There are\n",
    "several special characters, known as metacharacters, that control the\n",
    "behaviour of the rest of the regular expression. These metacharacters are\n",
    "listed in the following table.\n",
    "\n",
    "| Metacharacter | Meaning | Example |\n",
    "| ---- | ----- | ----- |\n",
    "| . |  Matches any character except a newline | `1.3` matches `123`, `1a3`, and `1#3` among others |\n",
    "| ^ | Matches sequence at the beginning of the line| `^Python` matches `Python` at the beginning of a line |\n",
    "| $ | Matches sequence at the end of the line | `Python$` matches `Python` at the end of a line |\n",
    "| * | Matches zero or more occurrences of a pattern | `12*3` matches `13`, `123`, `1223`, etc. |\n",
    "| + |  Matches one or more occurrences of a pattern | `12+3` matches `123`, `1223`, etc. |\n",
    "| ? |  Matches zero or one occurrences of a pattern | `12?3` matches `13` and `123` |\n",
    "| { }| Match repeated qualifier | `{m, n}` means match at least `m` and at most `n` occurrences | \n",
    "| [ ] | Used to specify a character class | `[a-z]` means match any lower case character |\n",
    "| \\ | Escape character | `\\w` means match and alphanumeric character, and `\\\\` means match a backslash |\n",
    "| &#124; | or operator | `A ` &#124; ` B` match either `A` or `B` |\n",
    "| ( ) | Grouping Operator | (a, b) |\n",
    "\n",
    "One additional point to remember is that inside a character class (i.e.,\n",
    "`[ ]`) many of these metacharacters lose their special meaning, and thus\n",
    "can be used to match themselves.\n",
    "\n",
    "Given a regular\n",
    "expression, the first task in Python is to compile the RE, which is done\n",
    "by using the `compile` method in the `re` module. This is demonstrated in\n",
    "the following code cell where we use a regular expression to find the\n",
    "element containing `CMI` to display our local airport.\n",
    "\n",
    "-----\n",
    "[re]: https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<airport name=\"University of Illinois-Willard\">\n",
      "<iata>CMI</iata>\n",
      "<city>Champaign/Urbana</city>\n",
      "<state>IL</state>\n",
      "<country>USA</country>\n",
      "<latitude>40.03925</latitude>\n",
      "<longitude>-88.27805556</longitude>\n",
      "</airport>\n"
     ]
    }
   ],
   "source": [
    "# We need the re module\n",
    "import re \n",
    "\n",
    "# Open and parse our XML document\n",
    "soup = BeautifulSoup(open('data/data.xml'), 'lxml')\n",
    "\n",
    "# Findelements containing the CMI string\n",
    "for el in soup.find_all(text=re.compile('CMI')):\n",
    "\n",
    "    # To get the entire airport element, we need to go \n",
    "    # up two levels in the DOM tree.\n",
    "    print(el.parent.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "Earlier in this notebook, we used the sqlite module to execute SQL queries. Now that you have run the cells in this notebook, go back to the relevant cells and make these changes. Be sure to understand how your changes impact the file input and output process.\n",
    "\n",
    "3. Try creating the Surf Shop database as a persistent database (i.e., not in memory).\n",
    "4. With the persistent Surf Shop database, execute queries to count the number of items in the store, and sort them into descending order by their description.\n",
    "56. The airport example demonstrate how easy it is to use the `to_sql` function on a Pandas DataFrame. Using any data set you choose (e.g., the Adult data or the Auto MPG data), read the data into a DataFrame and persist in a new database that you have created. Using the database, find all missing values.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "2. [BeautifulSoup][2] tutorial.\n",
    "3. The [scrapy][3] web parsing tool\n",
    "23. An older, but easy to follow blog article on using [BeautifulSoup][4] to parse a webpage.\n",
    "43. A tutorial notebook on [web scraping][43] with Python\n",
    "\n",
    "-----\n",
    "\n",
    "[2]: http://programminghistorian.org/lessons/intro-to-beautiful-soup\n",
    "[3]: http://scrapy.org\n",
    "[4]: https://programminghistorian.org/lessons/intro-to-beautiful-soup\n",
    "[43]: http://nbviewer.jupyter.org/url/www.unc.edu/%7Encaren/Lax-1.ipynb.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
