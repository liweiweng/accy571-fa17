{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Social Media: Twitter\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking for data to use for text data processing, one of the more popular data sources is [Twitter][tw]. In this notebook, we introduce the Twitter API, and demonstrate how to use the Twitter API from within a Python gram to acquire and process tweets, or Twitter messages. First, we review the mechanisms by which an application authenticates with Twitter. Next, we discuss different techniques for interacting with the twitter data stream by using the twitter API. Finally, we construct a tweet sentiment analysis pipeline before applying this pipeline to new tweets from a specific user.\n",
    "\n",
    "-----\n",
    "[tw]: https://www.twitter.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Python and Twitter](#Python-and-Twitter)\n",
    "\n",
    "[Reading Twitter Data](#Reading-Twitter-Data)\n",
    "\n",
    "[Obtaining Tweets](#Obtaining-Tweets)\n",
    "\n",
    "[Searching for Tweets](#Searching-for-Tweets)\n",
    "\n",
    "[Trending Topics](#Trending-Topics)\n",
    "\n",
    "[Twitter Text Analysis](#Twitter-Text-Analysis)\n",
    "\n",
    "- [Blind Testing](#Blind-Testing)\n",
    "- [Classifying New Tweets](#Classifying-New-Tweets)\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first include the notebook setup code and we define our _home_ directory.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we find our HOME directory\n",
    "home_dir = !echo $HOME\n",
    "\n",
    "# Define data directory\n",
    "home = home_dir[0] +'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Python and Twitter\n",
    "\n",
    "To work with the Twitter API from within a Python program, we need a Python library that wraps the official [Twitter API][twapi]. There are a number of different Python libraries that provide this capability, we will use the [tweepy][tpy] library, which is fairly popular and provides a fairly complete interface.\n",
    "\n",
    "The full Twitter API is large and robust (and continuous to evolve), for this course we will restrict our attention to several basic concepts, namely authenticating to Twitter, searching for Tweets, and digesting the messages.\n",
    "\n",
    "----\n",
    "[twapi]: https://dev.twitter.com\n",
    "[tpy]: http://www.tweepy.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python twitter library\n",
    "import tweepy as tw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Reading Twitter Data\n",
    "\n",
    "To read Twitter data, you need to first need to be a registered Twitter user and you need to create a new _Twitter Application_ in order to obtain credentials for connecting to Twitter and querying to the Twitter data. You create (and later manage) Twitter applications by visiting the [Twitter Application Management](https://apps.twitter.com) website.\n",
    "\n",
    "![Twitter App Sign-in](images/twitter-app-signin.png)\n",
    "\n",
    "At this point you need to authenticate with Twitter, if you are already logged in to Twitter on your computer (for instance by using the Twitter website) you should already be authenticated. If you are not authenticated, click the _sign in_ link to be directed to the Twitter sign in page where you can enter your credentials (if you do not have Twitter credentials, you will need to obtain a Twitter account to proceed).\n",
    "\n",
    "![Twitter Sign-in](images/twitter-signin.png)\n",
    "\n",
    "After you have been authenticated, you will be redirected to the Twitter apps page. If you have never created a Twitter application, you will have nothing listed. To create a new application, press the _Create New App_ button, as shown in the following screenshot.\n",
    "\n",
    "![Twitter Create App](images/twitter-create.png)\n",
    "\n",
    "This will open up the Twitter _Create an application_ webpage, where you need to supply some basic information for your Twitter application such as an application name, description, and website.\n",
    "\n",
    "![Twitter Application details](images/twitter-appdetails.png)\n",
    "\n",
    "Scroll to the bottom of this webpage where the **Developer Agreement** is located. Following this agreement, is a check box that you should click to signify you agree to be bound by the agreement (of course you should read this to be sure you do _agree_ with it first). Following this, press the _Create your Twitter application_ button as shown in the following screenshot.\n",
    "\n",
    "![Twitter Agree](images/twitter-agree.png)\n",
    "\n",
    "This will create your new application, and provide you with your application webpage, which will be similar to the following screenshot. \n",
    "\n",
    "![Twitter App-page](images/twitter-apppage.png)\n",
    "\n",
    "While you can control a number of application features from this webpage, the most important tasks to complete include:\n",
    "\n",
    "1. Change your application to _read-only_ in case it is set to read-write.\n",
    "\n",
    "2. Obtain the application **Consumer Key** and **Consumer Secret**.\n",
    "\n",
    "3. Obtain your personal **Access Token** and **Access Token Secret**.\n",
    "\n",
    "You should change your application read-only to ensure you don't accidentally send data out to Twitter. You change this by selecting the _Permissions_ tab and selecting _Read only_, shown in the following screenshot. To save this setting, scroll down this webpage and click the _Update Settings_ button at the bottom of the page.\n",
    "\n",
    "![Twitter Read Only Setting](images/twitter-ro.png)\n",
    "\n",
    "These credentials can be found by selecting the _Keys and Access Tokens_ tab, and scrolling down appropriately as shown in the following two screenshots.\n",
    "\n",
    "![Twitter Consumer Application Credentials](images/twitter-consume.png)\n",
    "\n",
    "![Twitter User Credentials](images/twitter-access.png)\n",
    "\n",
    "<font color='red'> Warning: Never share these credentials with others or they will be able to fully impersonate you on Twitter! </font>\n",
    "\n",
    "You can directly copy these credentials into your notebook, or, alternatively, save them into a file (for example by opening a terminal window and using `vim` to create a text file. In the rest of this notebook, I demonstrate this functionality by using my credentials, which I have saved into a file called `twitter.cred`. In this empty file, which is in your github repository, I have saved the following four credentials in order:\n",
    "\n",
    "1. Access Token\n",
    "2. Access Token Secret\n",
    "3. Consumer Key\n",
    "4. Consumer Secret\n",
    "\n",
    "If you are using git for source code management and control, you can inform `git` to ignore changes in the `twitter.cred` file by using the following command in the current module directory:\n",
    "\n",
    "```bash\n",
    "git update-index --assume-unchanged notebooks/twitter.cred \n",
    "```\n",
    "\n",
    "The following Code cell demonstrates how these credentials are read from the file and used to properly authenticate our application with Twitter.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Screen Name:  ProfBrunner\n",
      "Twitter Follower Count:  137\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "# Order: Access Token, Access Token Secret, Consumer Key, Consumer SecretAccess\n",
    "\n",
    "with open(\"twitter.cred\", 'r') as fin:\n",
    "    for line in fin:\n",
    "        if line[0] != '#': # Not a comment line\n",
    "            tokens.append(line.rstrip('\\n'))\n",
    "\n",
    "auth = tw.OAuthHandler(tokens[2], tokens[3])\n",
    "auth.set_access_token(tokens[0], tokens[1])\n",
    "\n",
    "api = tw.API(auth)\n",
    "\n",
    "user = api.me()\n",
    "\n",
    "print(\"Twitter Screen Name: \", user.screen_name)\n",
    "print(\"Twitter Follower Count: \", user.followers_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "If the previous Code cell runs without an error, you have successfully connected to Twitter. If you are new to Twitter and are not following anyone, you can instead display the user information for a different Twitter user. For example, the following code would display my Twitter information.\n",
    "\n",
    "```python\n",
    "user = api.get_user('ProfBrunner')\n",
    "```\n",
    "\n",
    "Replacing `ProfBrunner` with any valid Twitter user id will display their information. You can find examples by looking at those Twitter users you (or `ProfBrunner`) follow. Or, alternatively, you could chose a specific twitter account; for example, to analyze the _NY Times_ Twitter account you would use the following statement:\n",
    "\n",
    "```python\n",
    "user = api.get_user('NYTimes')\n",
    "```\n",
    "\n",
    "This is demonstrated in the following Code cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Screen Name:  nytimes\n",
      "Twitter Follower Count:  39979999\n",
      "\n",
      "This user follows:\n",
      "--------------\n",
      "jialynnyang\n",
      "johannabarr\n",
      "kevinmdraper\n",
      "itsjina\n",
      "Jan_Ransom\n",
      "mega2e\n",
      "bencasselman\n",
      "jimtankersley\n",
      "Tmgneff\n",
      "deborah_solomon\n",
      "luisferre\n",
      "a_symonds\n",
      "AnaSwanson\n",
      "uugwuu\n",
      "AustenProject\n",
      "porterthereport\n",
      "miwine\n",
      "MarcSantoraNYT\n",
      "Choire\n",
      "abscribe\n"
     ]
    }
   ],
   "source": [
    "user = api.get_user('nytimes')\n",
    "\n",
    "print(\"Twitter Screen Name: \", user.screen_name)\n",
    "print(\"Twitter Follower Count: \", user.followers_count)\n",
    "\n",
    "print(\"\\nThis user follows:\\n--------------\")\n",
    "for friend in user.friends():\n",
    "    print(friend.screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "At any point, you can return to your Twitter application management webpage to view your new application. You can now view and manage your existing application, or create a new application as shown in the following screenshot.\n",
    "\n",
    "![Twitter new app management](images/twitter-manage.png)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "\n",
    "To run the Twitter application in the preceding cells, you will need to register your own Twitter Application. To do so, complete the following steps.\n",
    "\n",
    "1. Create a New Twitter application.\n",
    "\n",
    "2. Save your Twitter credentials and Application credentials into the provided `twitter.cred` file.\n",
    "\n",
    "3. Run the _tweepy_ sample code to connect to Twitter and display your Twitter user information.\n",
    "\n",
    "Finally, try running the preceding code, but for someone famous (if you do not know the Twitter handle for someone famous, Google will be your friend). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Obtaining Tweets\n",
    "\n",
    "Once you have authenticated with Twitter, you can begin to [search the Twitter stream][stw] for tweets of interest. The easiest method to get started is to being with your own (or another specific Twitter user's) own Twitter feed. To access your own Twitter feed, you can simply use your `home_timeline` to retrieve your own Tweets or Tweets from those whom you follow. This is demonstrated in the following Code cell, where we display the `text` values from the ten most recent Tweets from our timeline.\n",
    "\n",
    "-----\n",
    "[stw]: https://dev.twitter.com/rest/public/search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilbert Classics (October 31): https://t.co/EyYnP81lxr\n",
      "Check out the introduction of Eager Execution for #TensorFlow! https://t.co/7lrozS96KO\n",
      "RT @TensorFlow: Introducing Eager Execution - an imperative, define-by-run interface for #TensorFlow! \n",
      "\n",
      "Learn more here → https://t.co/S3te…\n",
      "Midnight tomorrow at the latest time zone. :-) (retweeting as a PSA!) https://t.co/WjWw6y9hKu\n",
      "Utility functions are contentious so we agree on a mediocre implicit utility rather than pick a better explicit one. https://t.co/f7UwOPVNKQ\n",
      "Some of my Halloween comics all in one place!!! It's so exciting!!! AND SCARY!!! Going to use more exclamation poin… https://t.co/mYURr9gPNy\n",
      "@ApacheArrow 5/ There are some factual errors in this post. I will write a lengthier response. Compressed buffers i… https://t.co/DbPwwoYpEP\n",
      "@ApacheArrow 4/ One can also share / reuse of libraries of algorithms, which is hard now because systems have \"prop… https://t.co/7quS3Pi70j\n",
      "@ApacheArrow 3/ Being able to do zero-copy data access of arbitrarily large structured data sets is immensely power… https://t.co/Gj9PHtMGHg\n",
      "2/ @ApacheArrow supports a much higher diversity of workloads than analytic databases. Some analytic SQL systems may not need Arrow\n"
     ]
    }
   ],
   "source": [
    "for tweet in tw.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    print(tweet.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Searching for Tweets\n",
    "\n",
    "Twitter also provides the capability to search for specific tweets by using the Tweepy [`search` method][twse]. In this method, you supply a query string (and optional arguments) and are returned a list of Tweets. The query string should follow the [Twitter Search API][tsa], but basically you can search for specific text in a string by using the text of interest, you can search for a person by using the `@` character followed by their Twitter username, and hashtags by using the `#` character followed by the tag text.\n",
    "\n",
    "-----\n",
    "[twse]: http://docs.tweepy.org/en/stable/api.html#API.search\n",
    "[tsa]: https://dev.twitter.com/rest/public/search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet ID: 925360571003887617\n",
      "Tweeted by  dginev\n",
      "Created at  2017-10-31 13:55:27\n",
      "Location:  Twitter Web Client\n",
      "Tweet Text:  RT @pcmasuzzo: @Protohedgehog @azraiekv @kaveh1000 @PREreview_ @franknorman Text, data, code, version and much more. All this together is s…\n",
      "-------------------------\n",
      "Tweet ID: 925360141905543169\n",
      "Tweeted by  PREreview_\n",
      "Created at  2017-10-31 13:53:44\n",
      "Location:  Twitter for iPhone\n",
      "Tweet Text:  RT @pcmasuzzo: @Protohedgehog @azraiekv @kaveh1000 @PREreview_ @franknorman Text, data, code, version and much more. All this together is s…\n",
      "-------------------------\n",
      "Tweet ID: 925347965555818497\n",
      "Tweeted by  umalvarez\n",
      "Created at  2017-10-31 13:05:21\n",
      "Location:  Twitter for iPhone\n",
      "Tweet Text:  RT @sanad_maker: #julialang Here's my 2 cents on using julia for data science https://t.co/EqXGeBxpEZ :D\n",
      "-------------------------\n",
      "Tweet ID: 925330700160684032\n",
      "Tweeted by  schmidtjoff\n",
      "Created at  2017-10-31 11:56:45\n",
      "Location:  Twitter for Mac\n",
      "Tweet Text:  RT @pcmasuzzo: @Protohedgehog @azraiekv @kaveh1000 @PREreview_ @franknorman Text, data, code, version and much more. All this together is s…\n",
      "-------------------------\n",
      "Tweet ID: 925327361553371136\n",
      "Tweeted by  baursafi\n",
      "Created at  2017-10-31 11:43:29\n",
      "Location:  Twitter for Android\n",
      "Tweet Text:  @kjam Oh I am job searching, I just graduated from a data science bootcamp. Got a new MacBook Pro for it. Little di… https://t.co/jBPMc4QRnE\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Hash Tage search: term = '#python'\n",
    "# User search: term = '@nytimes'\n",
    "# Keyword search: term = 'data science'\n",
    "# Keyword and Sentiment: term ='data science :)' # Positive attitute\n",
    "\n",
    "term ='data science :)'\n",
    "num_tweets = 5\n",
    "\n",
    "for tweet in tw.Cursor(api.search, q=term).items(num_tweets):\n",
    "    # Process a single status\n",
    "    print(\"Tweet ID:\", tweet.id)\n",
    "    print('Tweeted by ', tweet.user.screen_name)\n",
    "    print(\"Created at \",tweet.created_at)\n",
    "    print(\"Location: \",tweet.source)\n",
    "    print('Tweet Text: ', tweet.text)\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can view the available attributes to display by using Python `dir` method to perform introspection. In the following Code cell we explicitly remove _class_ methods to minimize the display list and focus on the items of interest. After this, we display the Tweet in its raw JSON format by accessing the `_json` attribute.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '_max_id', '_since_id', 'append', 'clear', 'completed_in', 'copy', 'count',\n",
      "  'extend', 'ids', 'index', 'insert', 'max_id', 'next_results', 'parse', 'pop',\n",
      "  'query', 'refresh_url', 'remove', 'reverse', 'since_id', 'sort']\n"
     ]
    }
   ],
   "source": [
    "# Print out single tweet\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "\n",
    "tweets = api.search(q='Deloitte', rpp=1)\n",
    "\n",
    "pp.pprint([att for att in dir(tweets) if '__' not in att])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '_api', '_json', 'author', 'contributors', 'coordinates', 'created_at',\n",
      "  'destroy', 'entities', 'favorite', 'favorite_count', 'favorited', 'geo', 'id',\n",
      "  'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id',\n",
      "  'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "  'is_quote_status', 'lang', 'metadata', 'parse', 'parse_list', 'place',\n",
      "  'retweet', 'retweet_count', 'retweeted', 'retweeted_status', 'retweets',\n",
      "  'source', 'source_url', 'text', 'truncated', 'user']\n"
     ]
    }
   ],
   "source": [
    "# Pick a single tweet to analyze\n",
    "tweet = tweets[1]\n",
    "pp.pprint([att for att in dir(tweet) if '__' not in att])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'contributors': None,\n",
      "  'coordinates': None,\n",
      "  'created_at': 'Tue Oct 31 19:11:18 +0000 2017',\n",
      "  'entities': { 'hashtags': [],\n",
      "                'symbols': [],\n",
      "                'urls': [],\n",
      "                'user_mentions': [...]},\n",
      "  'favorite_count': 0,\n",
      "  'favorited': False,\n",
      "  'geo': None,\n",
      "  'id': 925440057456152576,\n",
      "  'id_str': '925440057456152576',\n",
      "  'in_reply_to_screen_name': None,\n",
      "  'in_reply_to_status_id': None,\n",
      "  'in_reply_to_status_id_str': None,\n",
      "  'in_reply_to_user_id': None,\n",
      "  'in_reply_to_user_id_str': None,\n",
      "  'is_quote_status': False,\n",
      "  'lang': 'es',\n",
      "  'metadata': {'iso_language_code': 'es', 'result_type': 'recent'},\n",
      "  'place': None,\n",
      "  'retweet_count': 47,\n",
      "  'retweeted': False,\n",
      "  'retweeted_status': { 'contributors': None,\n",
      "                        'coordinates': None,\n",
      "                        'created_at': 'Tue Oct 31 09:03:58 +0000 2017',\n",
      "                        'entities': {...},\n",
      "                        'favorite_count': 45,\n",
      "                        'favorited': False,\n",
      "                        'geo': None,\n",
      "                        'id': 925287218603806720,\n",
      "                        'id_str': '925287218603806720',\n",
      "                        'in_reply_to_screen_name': 'APDasociacion',\n",
      "                        'in_reply_to_status_id': 925284061588934656,\n",
      "                        'in_reply_to_status_id_str': '925284061588934656',\n",
      "                        'in_reply_to_user_id': 203003270,\n",
      "                        'in_reply_to_user_id_str': '203003270',\n",
      "                        'is_quote_status': False,\n",
      "                        'lang': 'es',\n",
      "                        'metadata': {...},\n",
      "                        'place': {...},\n",
      "                        'possibly_sensitive': False,\n",
      "                        'retweet_count': 47,\n",
      "                        'retweeted': False,\n",
      "                        'source': '<a href=\"http://twitter.com\" '\n",
      "                                  'rel=\"nofollow\">Twitter Web Client</a>',\n",
      "                        'text': '@fibesevilla @TEKNOSERVICE @BEONworldwide '\n",
      "                                '@Heineken_ES @Mediapost_Group @GroupMe '\n",
      "                                '@inmark_ES @Deloitte_ES… '\n",
      "                                'https://t.co/WWA9LKM0ZX',\n",
      "                        'truncated': True,\n",
      "                        'user': {...}},\n",
      "  'source': '<a href=\"http://twitter.com/download/iphone\" '\n",
      "            'rel=\"nofollow\">Twitter for iPhone</a>',\n",
      "  'text': 'RT @APDasociacion: @fibesevilla @TEKNOSERVICE @BEONworldwide '\n",
      "          '@Heineken_ES @Mediapost_Group @GroupMe @inmark_ES @Deloitte_ES '\n",
      "          '@iberinform @rf…',\n",
      "  'truncated': False,\n",
      "  'user': { 'contributors_enabled': False,\n",
      "            'created_at': 'Sun Feb 22 13:31:18 +0000 2015',\n",
      "            'default_profile': True,\n",
      "            'default_profile_image': False,\n",
      "            'description': '',\n",
      "            'entities': {...},\n",
      "            'favourites_count': 837,\n",
      "            'follow_request_sent': False,\n",
      "            'followers_count': 192,\n",
      "            'following': False,\n",
      "            'friends_count': 320,\n",
      "            'geo_enabled': False,\n",
      "            'has_extended_profile': False,\n",
      "            'id': 3052657827,\n",
      "            'id_str': '3052657827',\n",
      "            'is_translation_enabled': False,\n",
      "            'is_translator': False,\n",
      "            'lang': 'es',\n",
      "            'listed_count': 7,\n",
      "            'location': '',\n",
      "            'name': 'Sonia Ojeda Garrido',\n",
      "            'notifications': False,\n",
      "            'profile_background_color': 'C0DEED',\n",
      "            'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "            'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "            'profile_background_tile': False,\n",
      "            'profile_banner_url': 'https://pbs.twimg.com/profile_banners/3052657827/1453127536',\n",
      "            'profile_image_url': 'http://pbs.twimg.com/profile_images/783318152021172224/g_VWaq80_normal.jpg',\n",
      "            'profile_image_url_https': 'https://pbs.twimg.com/profile_images/783318152021172224/g_VWaq80_normal.jpg',\n",
      "            'profile_link_color': '1DA1F2',\n",
      "            'profile_sidebar_border_color': 'C0DEED',\n",
      "            'profile_sidebar_fill_color': 'DDEEF6',\n",
      "            'profile_text_color': '333333',\n",
      "            'profile_use_background_image': True,\n",
      "            'protected': False,\n",
      "            'screen_name': 'SoniaOjedaSOG',\n",
      "            'statuses_count': 719,\n",
      "            'time_zone': None,\n",
      "            'translator_type': 'none',\n",
      "            'url': None,\n",
      "            'utc_offset': None,\n",
      "            'verified': False}}\n"
     ]
    }
   ],
   "source": [
    "# We can display the message data in JSON format\n",
    "pp.pprint(tweet._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Trending Topics\n",
    "\n",
    "Twitter tracks tweet data to identify topics or people that are being frequently mentioned, which is known as [_trending_][twtr]. The Twitter API enables an application to obtain a list of currently trending topics. These topics include metadata that can be used to learn more about trending topics. One component of the metadata is the physical location of the trending topic. This location is encoded as a [**WOEID**][woeid], which is a Yahoo developed standard that is short for _where on the earth ID_. In the first Code cell below, we demonstrate obtaining the locations of currently trending topics before displaying these physical locations. In the second Code cell, we display the complete metadata for one location, which can be used to obtain a list of trending topics for a particular location on Earth, via the WOEID. Note that since trending topics change, this notebook will provide different results when run at different times.\n",
    "\n",
    "----\n",
    "[twtr]: https://dev.twitter.com/rest/reference/get/trends/available\n",
    "[woeid]: https://developer.yahoo.com/geo/geoplanet/guide/concepts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOEID Code (2972): Winnipeg, Canada\n",
      "WOEID Code (3369): Ottawa, Canada\n",
      "WOEID Code (3444): Quebec, Canada\n",
      "WOEID Code (3534): Montreal, Canada\n",
      "WOEID Code (4118): Toronto, Canada\n",
      "WOEID Code (8676): Edmonton, Canada\n",
      "WOEID Code (8775): Calgary, Canada\n",
      "WOEID Code (9807): Vancouver, Canada\n",
      "WOEID Code (12723): Birmingham, United Kingdom\n",
      "WOEID Code (12903): Blackpool, United Kingdom\n",
      "WOEID Code (13383): Bournemouth, United Kingdom\n",
      "WOEID Code (13911): Brighton, United Kingdom\n",
      "WOEID Code (13963): Bristol, United Kingdom\n",
      "WOEID Code (15127): Cardiff, United Kingdom\n",
      "WOEID Code (17044): Coventry, United Kingdom\n",
      "WOEID Code (18114): Derby, United Kingdom\n",
      "WOEID Code (19344): Edinburgh, United Kingdom\n",
      "WOEID Code (21125): Glasgow, United Kingdom\n",
      "WOEID Code (25211): Hull, United Kingdom\n"
     ]
    }
   ],
   "source": [
    "# Returns a JSON object that contains (a large number of) locations \n",
    "# that are currently trending.\n",
    "\n",
    "top_display = 20\n",
    "trending = api.trends_available()\n",
    "\n",
    "# We skip first value, which is entry for the World in JSON.\n",
    "for trend in trending[1:top_display]:\n",
    "    print('WOEID Code ({2:d}): {0}, {1}'.format(trend['name'], \\\n",
    "                                                trend['country'], trend['woeid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'country': 'United Kingdom',\n",
      "  'countryCode': 'GB',\n",
      "  'name': 'Blackpool',\n",
      "  'parentid': 23424975,\n",
      "  'placeType': {'code': 7, 'name': 'Town'},\n",
      "  'url': 'http://where.yahooapis.com/v1/place/12903',\n",
      "  'woeid': 12903}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(trending[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK Trends\n",
      "----------\n",
      "  #HappyHalloween2017\n",
      "  Bex Bailey\n",
      "  Trick or Treaters\n",
      "  Ian Katz\n",
      "  #ROMCHE\n",
      "  Wendy Williams\n",
      "  #ShipTease\n",
      "  McTominay\n",
      "  #IOEdebates\n",
      "  #StirlingPrize\n"
     ]
    }
   ],
   "source": [
    "# We can use a WOEID to find location specific trends.\n",
    "# Here we use the WOEID for the UK (from previous example)\n",
    "\n",
    "top_display = 10\n",
    "\n",
    "print(\"UK Trends\")\n",
    "print(10*'-')\n",
    "\n",
    "for trends in api.trends_place(id = 23424975):\n",
    "    for trend in trends[\"trends\"][:top_display]:\n",
    "        print(\"  {0:s}\".format(trend[\"name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "\n",
    "In the preceding cells, we used the twitter API to obtain tweets and to identify trending topics. Now that you have run the notebook, try making the following changes.\n",
    "\n",
    "1. Pick a particular twitter user and search their twitter stream.\n",
    "2. Pick a different location from the trending topics location list, and identify trending topics from a different WOEID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## Twitter Text Analysis\n",
    "\n",
    "We can now develop a text analysis project that uses twitter data. To simplify the application, we will use the NLTK twitter corpus. Otherwise, we would need a separate notebook to obtain the necessary tweets (because of the twitter rate limitation). The [NLTK twitter corpus][ntw] includes thirty thousand tweets retrieved from the twitter streaming API, the data have been cached on our course JupyterHub server. The tweets were explicitly selected from a recent election in the United Kingdom and one-third of them have been classified into positive or negative (with equal numbers of each). With these tweets, we can build a classification pipeline to perform sentiment analysis on twitter data.\n",
    "\n",
    "In the following Code cells, we first obtain the tweets, build the NumPy arrays for our classification pipeline, before constructing and testing this simple sentiment analysis text analysis application.\n",
    "\n",
    "-----\n",
    "[ntw]: http://www.nltk.org/howto/twitter.html#Using-a-Tweet-Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 Positive Tweets\n",
      "5000 Negative Tweets\n"
     ]
    }
   ],
   "source": [
    "tws = nltk.corpus.twitter_samples\n",
    "\n",
    "pos_tweets = np.array(tws.strings('positive_tweets.json'))\n",
    "neg_tweets = np.array(tws.strings('negative_tweets.json'))\n",
    "\n",
    "pos_labels = np.ones(pos_tweets.shape[0])\n",
    "neg_labels = np.zeros(neg_tweets.shape[0])\n",
    "\n",
    "targets = np.concatenate((pos_labels, neg_labels), axis=0)\n",
    "data = np.concatenate((pos_tweets, neg_tweets), axis = 0)\n",
    "\n",
    "print('{0} Positive Tweets'.format(pos_tweets.shape[0]))\n",
    "print('{0} Negative Tweets'.format(neg_tweets.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We will employ 75% of the data for training, with 25% held out for validation. The classification pipeline will use a simply tokenizer to build a document-term matrix before applying a Naive Bayes classifier. The tokenizer will use _English_ stop words, will convert the text to all lowercase, and includes both unigrams and bigrams. Overall, this simple classification pipeline gives reasonable results.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, targets, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.73      0.79      0.76      1240\n",
      "   Negative       0.78      0.71      0.74      1260\n",
      "\n",
      "avg / total       0.75      0.75      0.75      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pclf = Pipeline(tools)\n",
    "\n",
    "\n",
    "# Lowercase, English Stop Words, and unigrams and bigrams.\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,2), \\\n",
    "                cv__lowercase=True)\n",
    "\n",
    "pclf.fit(x_train, y_train)\n",
    "y_pred = pclf.predict(x_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = ['Positive', 'Negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Blind Testing\n",
    "\n",
    "We can use the remaining twenty thousand tweets in the NLTK corpus for blind testing. We first obtain the tweets as a NumPy array, before applying our sentiment analysis pipeline. Finally, we display the relative numbers of positive and negative classifications and we display examples of both positive and negative classified tweets.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_tweets = np.array(tws.strings('tweets.20150430-223406.json'))\n",
    "unknown_pred = pclf.predict(unknown_tweets)\n",
    "\n",
    "unknown_pos = unknown_tweets[unknown_pred == 1]\n",
    "unknown_neg = unknown_tweets[unknown_pred == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 tweets to classify.\n",
      "8508 tweets classified as positive.\n",
      "11492 tweets classified as negative.\n",
      "---------------------------------------------------------------------------\n",
      "Sample Positve Tweet:\n",
      "---------------------------------------------------------------------------\n",
      "\"David Cameron: smooth, smiley but unconvincing\" #bbcqt http://t.co/mJ2ZkX1TjB\n",
      "---------------------------------------------------------------------------\n",
      "Sample Negatve Tweet:\n",
      "---------------------------------------------------------------------------\n",
      "RT @DouglasDaniel: Miliband's new line 'if you don't vote Labour in Scotland I will punish you by letting the Tories in'.\n"
     ]
    }
   ],
   "source": [
    "tweet_idx = 101\n",
    "\n",
    "print(f'{unknown_tweets.shape[0]} tweets to classify.')\n",
    "print(f'{unknown_pos.shape[0]} tweets classified as positive.')\n",
    "print(f'{unknown_neg.shape[0]} tweets classified as negative.')\n",
    "\n",
    "print(75*'-')\n",
    "print('Sample Positve Tweet:')\n",
    "print(75*'-')\n",
    "print(unknown_pos[tweet_idx])\n",
    "\n",
    "print(75*'-')\n",
    "print('Sample Negatve Tweet:')\n",
    "print(75*'-')\n",
    "print(unknown_neg[tweet_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Classifying New Tweets\n",
    "\n",
    "We can now combine everything covered in this notebook in order to apply our trained sentiment analysis pipeline on new twitter data. For this, we pick _random_ user, in this case CNN Political Twitter Feed (note this wasn't completely random. The training data was obtained from a similar feeds). We first obtain tweets from this user before creating the NumPy arrays to use with our scikit learn classifier. Finally, we display the results of this sentiment analysis classifier. Note, since the tweets will change over time, the results presented in this notebook will also change.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained a sample of 94 tweets\n"
     ]
    }
   ],
   "source": [
    "newtweets = api.user_timeline(screen_name='@CNNPolitics', include_rts=False, count=100)\n",
    "                           \n",
    "print(f'We obtained a sample of {len(newtweets)} tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "for tweet in newtweets:\n",
    "    messages.append(tweet.text)\n",
    "    \n",
    "new_tweets = np.array(messages)\n",
    "new_pred = pclf.predict(new_tweets)\n",
    "\n",
    "new_pos = new_tweets[new_pred == 1]\n",
    "new_neg = new_tweets[new_pred == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 tweets to classify.\n",
      "49 tweets classified as positive.\n",
      "45 tweets classified as negative.\n",
      "---------------------------------------------------------------------------\n",
      "Sample Positve Tweet:\n",
      "---------------------------------------------------------------------------\n",
      "John Kelly is a lot more like Donald Trump than we thought | Analysis by CNN's Chris Cillizza… https://t.co/bvkE1zGBpl\n",
      "---------------------------------------------------------------------------\n",
      "Sample Negatve Tweet:\n",
      "---------------------------------------------------------------------------\n",
      "Cardin: \"If the President tried to compromise\" Mueller's investigation, \"I think the American people would stand up\" https://t.co/KFu3ZMt8Lg\n"
     ]
    }
   ],
   "source": [
    "# Pick a tweet index\n",
    "tweet_idx = 13\n",
    "\n",
    "print(f'{new_tweets.shape[0]} tweets to classify.')\n",
    "print(f'{new_pos.shape[0]} tweets classified as positive.')\n",
    "print(f'{new_neg.shape[0]} tweets classified as negative.')\n",
    "\n",
    "print(75*'-')\n",
    "print('Sample Positve Tweet:')\n",
    "print(75*'-')\n",
    "print(new_pos[tweet_idx])\n",
    "\n",
    "print(75*'-')\n",
    "print('Sample Negatve Tweet:')\n",
    "print(75*'-')\n",
    "print(new_neg[tweet_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "\n",
    "In the preceding cells, we build a sentiment analysis classification pipeline by using the NLTK corpus before applying it to new tweets. Now that you have run the notebook, try making the following changes.\n",
    "\n",
    "1. Modify the pipeline parameters, for example, apply stemming, change the `max_features`, or the number of n-grams. Can you improve the classification results on the validation data?\n",
    "\n",
    "2. Change the type of classification algorithm (e.g., random forest or SVC with regularization). Can you improve the classification results on the validation data?\n",
    "\n",
    "3. Using your new classification pipeline, examine the performance on the current twitter user. Look at other tweets, does the performance improve?\n",
    "\n",
    "4. Try classifying tweets from a different user, either an _election_ type feed or a popular figure. By looking at select tweets and their classification, comment on how your classifier performs?\n",
    "\n",
    "Finally, why do you think the classification pipeline performs in the manner that is does (i.e., why are some tweets classified negative/positive)? Feel free to use the class forums.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. Wikipedia article on [twitter][wt]  \n",
    "1. Wikipedia article on [Social Media][wsm]  \n",
    "1. Twitter, [official documentation][tod]  \n",
    "1. Map of a [twitter status object][mtso]  \n",
    "1. [Tweepy][twd]: Python Twitter client documentation (Getting Started and Authentication Tutorial)  \n",
    "1. [Using nltk][unt] with twitter (note uses a different twitter client library)  \n",
    "1. Blog demonstrating [twitter access via URLs][tu]  \n",
    "1. Blog article on collecting tweets by using [streaming api][tsa]  \n",
    "1. **Chapter 1: Mining Twitter** from _Mining the Social Web_, [Jupyter notebook][msw1]  \n",
    "1. **Chapter 9: Twitter Cookbook** from _Mining the Social Web_, [Jupyter notebook][msw1]  \n",
    "\n",
    "-----\n",
    "\n",
    "[wt]: https://en.wikipedia.org/wiki/Twitter\n",
    "[wsm]: https://en.wikipedia.org/wiki/Social_media\n",
    "\n",
    "[twd]: http://tweepy.readthedocs.org\n",
    "[tod]: https://dev.twitter.com/overview/documentation\n",
    "\n",
    "[unt]: http://www.nltk.org/howto/twitter.html\n",
    "\n",
    "[tsa]: http://badhessian.org/2012/10/collecting-real-time-twitter-data-with-the-streaming-api/\n",
    "[tu]: http://nealcaren.web.unc.edu/pizza-twitter-and-apis/\n",
    "\n",
    "[msw1]: https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/blob/master/ipynb/Chapter%201%20-%20Mining%20Twitter.ipynb\n",
    "[msw9]: https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/blob/master/ipynb/Chapter%209%20-%20Twitter%20Cookbook.ipynb\n",
    "[mtso]: http://online.wsj.com/public/resources/documents/TweetMetadata.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
