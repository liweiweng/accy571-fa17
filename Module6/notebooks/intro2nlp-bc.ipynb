{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP: Basic Concepts\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this IPython Notebook, we build on the [text analysis][w7i] concepts presented previously to dive more deeply into text analysis. Specifically, we will move beyond simple tokenization to leverage the semantic information contained in ordering and arrangement of text data to gain new insights. We will start by exploring alternative tokenization techniques provided by the NLTK library before delving into part-of-speech tagging and named entity recognition. \n",
    "\n",
    "We begin by parsing a simple text document that contains the course description for INFO 490 SP16 (i.e., this course). First, we will employ a sentence tokenizer, before changing to word, whitespace, and  word/punctuation tokenizers.\n",
    "\n",
    "-----\n",
    "\n",
    "[w7i]: ../../Week7/index.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[N-Grams](#N-Grams)\n",
    "\n",
    "[N-Gram Classification](#N-Gram-Classification)\n",
    "\n",
    "[Stemming](#Stemming)\n",
    "\n",
    "[Clustering Analysis](#Clustering-Analysis)\n",
    "\n",
    "[Dimension-Reduction](#Dimension-Reduction)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first include the notebook setup code and we define our _home_ directory.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we find our HOME directory\n",
    "home_dir = !echo $HOME\n",
    "\n",
    "# Define data directory\n",
    "home = home_dir[0] +'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/data_scientist/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6a77d62289ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Tokenize and display results. Also display one representative sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msnts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0} sentances in course description'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/data_scientist/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "# As a text example, we use the course description for INFO490  SP16.\n",
    "info_course = ['Advanced Data Science: This class is an asynchronous, online course.', \n",
    "               'This course will introduce advanced data science concepts by building on the foundational concepts presented in INFO 490: Foundations of Data Science.', \n",
    "               'Students will first learn how to perform more statistical data exploration and constructing and evaluating statistical models.', \n",
    "               'Next, students will learn machine learning techniques including supervised and unsupervised learning, dimensional reduction, and cluster finding.', \n",
    "               'An emphasis will be placed on the practical application of these techniques to high-dimensional numerical data, time series data, image data, and text data.', \n",
    "               'Finally, students will learn to use relational databases and cloud computing software components such as Hadoop, Spark, and NoSQL data stores.', \n",
    "               'Students must have access to a fairly modern computer, ideally that supports hardware virtualization, on which they can install software.', \n",
    "               'This class is open to sophomores, juniors, seniors and graduate students in any discipline who have either taken a previous INFO 490 data science course or have received instructor permission.']\n",
    "\n",
    "text = \" \".join(info_course)\n",
    "\n",
    "# Tokenize and display results. Also display one representative sentence\n",
    "from nltk import sent_tokenize\n",
    "snts = sent_tokenize(text)\n",
    "print('{0} sentances in course description'.format(len(snts)))\n",
    "print(40*'-')\n",
    "print(snts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by words, display results, and a representive section of words\n",
    "from nltk import word_tokenize\n",
    "wtks = word_tokenize(text)\n",
    "\n",
    "print('{0} words in course description'.format(len(wtks)))\n",
    "print(40*'-')\n",
    "\n",
    "# Display the tokens\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "\n",
    "pp.pprint(wtks[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "wtks = tokenizer.tokenize(text)\n",
    "\n",
    "print('{0} words in course description (WS Tokenizer)'.format(len(wtks)))\n",
    "print(40*'-')\n",
    "\n",
    "pp.pprint(wtks[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "wtks = tokenizer.tokenize(text)\n",
    "\n",
    "print('{0} words in course description (WP Tokenizer)'.format(len(wtks)))\n",
    "print(40*'-')\n",
    "\n",
    "pp.pprint(wtks[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Collocations\n",
    "\n",
    "We previously discussed using multiple, adjacent words, which is known\n",
    "as n-grams (e.g., bigrams or trigrams). We can also build\n",
    "[collocations][nc], where we use NLTK to grab n-grams, but now with the\n",
    "possibility of applying filters, such as a minimum frequency of\n",
    "occurrence. We can employ an association measure, such as the [pointwise\n",
    "mutual information][wpmi] (PMI), to compute the importance of a\n",
    "collocation. PMI quantifies the likelihood of two words occurring together\n",
    "in a document to their chance superposition (from their individual\n",
    "distribution in the document). Thus, a PMI close to one implies two\n",
    "words almost always occur together, while a PMI close to zero implies\n",
    "two words are nearly independent and rarely occur together.\n",
    "\n",
    "-----\n",
    "[nc]: http://www.nltk.org/howto/collocations.html\n",
    "[wpmi]: https://en.wikipedia.org/wiki/Pointwise_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_bgs = 10\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(wtks)\n",
    "bgs = finder.nbest(bigram_measures.pmi, top_bgs)\n",
    "\n",
    "print('Best {0} bi-grams in course description (WP Tokenizer)'.format(top_bgs))\n",
    "print(50*'-')\n",
    "\n",
    "ppf = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=False)\n",
    "ppf.pprint(bgs)\n",
    "\n",
    "print(50*'-')\n",
    "print('Best {0} bi-grams occuring more than once in course description (WP Tokenizer)'.format(top_bgs))\n",
    "print(50*'-')\n",
    "\n",
    "finder.apply_freq_filter(2)\n",
    "bgs = finder.nbest(bigram_measures.pmi, top_bgs)\n",
    "ppf.pprint(bgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(wtks)\n",
    "tgs = finder.nbest(trigram_measures.pmi, top_bgs)\n",
    "\n",
    "print('Best {0} tri-grams in course description (WP Tokenizer)'.format(top_bgs))\n",
    "print(50*'-')\n",
    "\n",
    "ppf = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=False)\n",
    "ppf.pprint(tgs)\n",
    "\n",
    "print(50*'-')\n",
    "print('Best tri-grams occuring more than once in course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "finder.apply_freq_filter(2)\n",
    "tgs = finder.nbest(bigram_measures.pmi, top_bgs)\n",
    "ppf.pprint(tgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Tagging\n",
    "\n",
    "The simplest approach to text analysis is the bag-of-words model, where\n",
    "we simply identify the words (or tokens) present in a set of documents.\n",
    "In order to move beyond this model, we need to include additional\n",
    "information with each word. For example, the word _duck_ can mean the\n",
    "bird or it can mean the action. More generally, this concept when\n",
    "applied to multiple words is known as a [garden path sentences][wgps]. \n",
    "\n",
    "In the bag of word model, the difference between these two meanings (of\n",
    "the word _duck_) is lost. By associating information about the context\n",
    "or the grammatical nature of a word, however, these different use cases\n",
    "can be distinguished. The mechanism by which this is done is known as\n",
    "tagging. A tag can be used to identify the grammatical nature of a word,\n",
    "like _noun_ or _verb_, or it can be other information, including\n",
    "associations with other words in the text. In the following code blocks,\n",
    "we first introduce a _DefaultTagger_, which associates a tag of our\n",
    "choosing with words. Afterwards, we use the NLTK built-in Part of\n",
    "Speech (POS) and Named Entity Recognition (NER) taggers.\n",
    "\n",
    "-----\n",
    "[wgps]: https://en.wikipedia.org/wiki/Garden_path_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tag = 'INFO'\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "default_tagger = DefaultTagger(a_tag)\n",
    "tgs = default_tagger.tag(wtks)\n",
    "\n",
    "print('Tagged course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "pp.pprint(tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Part of Speech Tagging\n",
    "\n",
    "Part of speech (PoS) simply refers to the grammatical properties of a word.\n",
    "While this might seem simple, given the diversity of languages (and even\n",
    "variations within a single language), this topic quickly becomes quite\n",
    "substantial. As a result, there are a number of possible approaches. In\n",
    "the next two code cells, we first demonstrate a simple PoS that labels\n",
    "only basic text components such as _Noun_, _Verb_, or _Adjective_,\n",
    "before moving to a more complex PoS that labels a wider range of text\n",
    "components, which can also establish grammatical relationships between\n",
    "multiple words.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "ptgs = pos_tag(wtks, tagset='universal')\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer/Univesal Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(ptgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "PoS tags can be much more complex, as shown in the following code cell.\n",
    "The specific tags depend on the selected tagset, by default NLTK now\n",
    "uses a [_PerceptronTagger_][pt], which quickly generates a set of tagged\n",
    "grammatical constructs.\n",
    "\n",
    "----\n",
    "[pt]: http://spacy.io/blog/part-of-speech-POS-tagger-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptgs = pos_tag(wtks)\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer/Default Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(ptgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) classifies (or recognizes) chunks of text\n",
    "that refer to pre-defined categories (or named entities). These chunks\n",
    "can be one or more words, and the categories can be names of people,\n",
    "organizations, locations, or other types of entities. For example, in\n",
    "the following sentence:\n",
    "\n",
    "> Edward is a graduate student enrolled at the University of Illinois.\n",
    "\n",
    "_Edward_ is a person and _University of Illinois_ is an organization.\n",
    "NLTK can be used to identify named entities, generally following a part\n",
    "of speech tagging (to clarify different uses of words that otherwise\n",
    "might cause confusion). In the following code cell, we demonstrate NER by\n",
    "using NLTK to identify named entities in the course description text.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "nrcs = ne_chunk(pos_tag(wtks))\n",
    "\n",
    "print(50*'-')\n",
    "print('NER tagged course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "ppf.pprint(nrcs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Corpus\n",
    "\n",
    "A corpus is simply a collection of documents. In the case of Natural\n",
    "Language Processing, however, a corpus can include additional\n",
    "information for both part of speech tagging and named entity\n",
    "recognition. The NLTK library includes several corpuses, including the\n",
    "Penn Treebank, Brown, and Wordnet. In the rest of this notebook, we\n",
    "introduce the first two corpuses; the Wordnet corpus is introduced in\n",
    "[Introduction to NLP: Semantic Analysis][l3] notebook.\n",
    "\n",
    "###  Penn Treebank\n",
    "\n",
    "The [Penn Treebank project][ptbp] is an effort to annotate text, into a\n",
    "linguistic structure. This structure is generally in the form of a\n",
    "[tree][wt], within which the different components of a sentence are\n",
    "organized. This process includes a [part of speech tagging][ptpos]. We\n",
    "demonstrate the use of the Penn Treebank with NLTK in the next few code\n",
    "cells, where we tokenize text by using a Penn Treebank standard sentence\n",
    "and word tokenizer, and tagged sentence and word tokenizers. Finally, we\n",
    "introduce the `UnigramTagger`, which can be trained on a given corpus to\n",
    "tokenize and tag unigrams in a new document (or set of documents).\n",
    "\n",
    "-----\n",
    "[ptbp]: https://www.cis.upenn.edu/~treebank/\n",
    "[ptpos]: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "[wt]: https://en.wikipedia.org/wiki/Treebank\n",
    "[l3]: intro2nlp-sa.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "print('Penn Treebank tagged text.')\n",
    "print(80*'-')\n",
    "\n",
    "print('Words:     ', end='')\n",
    "pp.pprint(treebank.words()[:18])\n",
    "print(80*'-')\n",
    "\n",
    "print('Setnences: ', end='')\n",
    "pp.pprint(treebank.sents()[0])\n",
    "print(80*'-')\n",
    "\n",
    "print('Tagged Words: ')\n",
    "pp.pprint(treebank.tagged_words()[:18])\n",
    "print(80*'-')\n",
    "\n",
    "print('Tagged Sentances: ')\n",
    "pp.pprint(treebank.tagged_sents()[0])\n",
    "print(80*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "pt_tagger = UnigramTagger(treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_tgs = pt_tagger.tag(wtks)\n",
    "\n",
    "print('Penn Treebank tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(pt_tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Brown Corpus\n",
    "\n",
    "The [Brown Corpus][wbc] has over one million tagged words, and was\n",
    "originally published in 1967. The corpus itself is composed of 500\n",
    "samples, spread over fifteen different genres, of English-language text\n",
    "compiled from works published in 1961. NLTK provides the Brown Corpus,\n",
    "which can be used to tag new documents, as shown below.\n",
    "\n",
    "----\n",
    "[wbc]: https://en.wikipedia.org/wiki/Brown_Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "b_tagger = UnigramTagger(brown.tagged_sents(brown.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tgs = b_tagger.tag(wtks)\n",
    "\n",
    "print('Brown tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(b_tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Linking Taggers\n",
    "\n",
    "\n",
    "In the previous examples, certain words were left untagged or tagged\n",
    "with `None` (such as _online_ or _asynchronous_). Since language evolves\n",
    "over time, an older corpus might miss words, or they may simply be\n",
    "incomplete. To handle these cases, NLTK enables taggers to be linked.\n",
    "Thus a general tagger can be applied, such as the Brown Corpus,\n",
    "after which a second tagger can be applied to increase the number of\n",
    "words tagged. This is a common application area for a _DefaultTagger_,\n",
    "which can be used to assign a specific tag to any element missed by\n",
    "another tagger. We demonstrate this concept below, by linking the Brown\n",
    "Corpus tagger with our earlier Default tagger.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can link taggers\n",
    "\n",
    "b_tagger._taggers = [b_tagger, default_tagger]\n",
    "\n",
    "b_tgs = b_tagger.tag(wtks)\n",
    "\n",
    "print('Brown tagged course description (WP Tokenizer/Linked Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(b_tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Tagged Text Extraction\n",
    "\n",
    "For some text analysis projects, we might want to restrict words (or\n",
    "tokens) to specific tags. For example, we might prefer to only use\n",
    "_Nouns_, _Primary Verbs_, or _Adjectives_ for text classification. To\n",
    "extract only terms that meet these conditions, we can tag the text, and\n",
    "apply a regular expression to the tagged tokens, as shown in the\n",
    "following code cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# NN matchs NN|NNS|NNP|NNPS\n",
    "rgxs = re.compile(r\"(JJ|NN|VBN|VBG)\")\n",
    "\n",
    "ptgs = pos_tag(wtks)\n",
    "trms = [tkn[0] for tkn in ptgs if re.match(rgxs, tkn[1])]\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "pp.pprint(ptgs[:13])\n",
    "print(60*'-')\n",
    "print('POS tagged course description (WP Tokenizer/RegEx applied)')\n",
    "print(60*'-')\n",
    "pp.pprint(trms[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we introduced several basic NLP concepts,\n",
    "including tagging, Part of Speech, and Named Entity Recognition. Now\n",
    "that you have run the Notebook, go back and make the following changes\n",
    "to see how the results change.\n",
    "\n",
    "1. Change from a Unigram tagger to a Bigram Tagger. How do you results\n",
    "change?\n",
    "2. Replace the initial text with a longer document (you can use a text\n",
    "from within NLTK or a freely available text from _Project Gutenberg_).\n",
    "Apply more restrictive filters (i.e., higher frequencies) to the bigrams\n",
    "and trigrams, do your results make sense?\n",
    "3. Try using regular expressions to restrict tokens in the NLTK movie\n",
    "review data set to Nouns, Verbs, Adjectives, and Adverbs. Use these\n",
    "tokens to perform Sentiment Analysis on these movie review data. Are the\n",
    "results better or worse than with all words?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used feature selection to identify the most important features in our simple classification pipeline. Now that you have run the Notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the vectorizer to change the case of all words an to employ stemming. How do the results (tokens) change?\n",
    "\n",
    "2. Change the classification algorithm to a more accurate method. How do the results change? How does the computational time change?\n",
    "\n",
    "Finally, what do the list of tokens say about the fact we did not remove headers or footers from the newsgroup postings? Feel free to comment on these questions in the course forum.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. [XML Tutorial][1] by W3Schools\n",
    "3. [SVG Tutorial][3] by W3Schools\n",
    "4. The [ColorBrewer2][cb2] website\n",
    "\n",
    "-----\n",
    "\n",
    "[1]: http://www.w3schools.com/xml/default.asp\n",
    "[3]: http://www.w3schools.com/svg/default.asp\n",
    "[cb2]: http://colorbrewer2.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
