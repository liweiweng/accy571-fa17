{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP: Semantic Analysis\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most elusive goals in natural language processing is the identification of semantic meaning from text. In this case, we don't want to simply use text to identify topics or classify documents. Instead we want to use the relationships between words in documents to gain insight from context. One simple way to understand this concept is to see that when words occur in similar arrangements or patterns across documents, the pattern conveys meaning beyond the simple appearance of the words themselves. \n",
    "\n",
    "In this notebook, we explore semantic analysis. First we use the wordnet corpus to identify similar words based on pre-defined relationships. Second, we use the _gensim_ library to create topic models that can be used to compute similarity measures based on the inherent patterns of words within a corpus. Finally, we explore the _word2vec_ algorithm, where a neural network is trained on a large corpus to identify relationships between words and phrase.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Wordnet](#N-Grams)\n",
    "\n",
    "- [Word Similarities](#Word-Similarities)\n",
    "\n",
    "[Gensim](#Gensim)\n",
    "\n",
    "[Word2Vec](#Word2Vec)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first define our _home_ directory.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we find our HOME directory\n",
    "home_dir = !echo $HOME\n",
    "\n",
    "# Define data directory\n",
    "home = home_dir[0] +'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Wordnet\n",
    "\n",
    "[Wordnet][wdn] is an English lexical database that groups words into synsets, which is shorthand for synonym sets. The database was created under at Princeton University and has been distributed with an open license. Given its nature, it is a different corpus than the Treebank or Brown corpora analyzed in the Topic Modeling notebook. A wordnet entry can have multiple definitions for a word, associated synonyms, lemmas, and other information that can be used to algorithmically identify relationships between words. In the next few Code cells we explore the wordnet corpus, before moving on to using it to compute word similarities.\n",
    "\n",
    "-----\n",
    "[wdn]: https://en.wikipedia.org/wiki/WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 total entries in synonym ring for drive. Only showing top {max_entries} entries.\n",
      "----------------------------------------------------------------------\n",
      "drive (n): the act of applying force to propel something\n",
      "----------------------------------------------------------------------\n",
      "drive (n): a mechanism by which force or power is transmitted in a machine\n",
      "----------------------------------------------------------------------\n",
      "campaign (n): a series of actions advancing a principle or tending toward a particular end\n",
      "----------------------------------------------------------------------\n",
      "driveway (n): a road leading up to a private house\n",
      "----------------------------------------------------------------------\n",
      "drive (n): the trait of being highly motivated\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Explore WordNet synonym rings\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# We limit the number of items in ring to display\n",
    "max_entries = 5\n",
    "\n",
    "# Choose a word, change this to see different results.\n",
    "the_word = 'drive'\n",
    "the_synsets = wn.synsets(the_word)\n",
    "\n",
    "# Display summary stats\n",
    "print(f'{len(the_synsets)} total entries ', end='')\n",
    "print(f'in synonym ring for {the_word}. ', end='')\n",
    "print('Only showing top {max_entries} entries.')\n",
    "print(70*'-')\n",
    "\n",
    "# Step through ring and display data\n",
    "for synset in the_synsets[:max_entries]:\n",
    "    vals = synset.name().split('.')\n",
    "    print(f'{vals[0]} ({vals[1]}): ', end='')\n",
    "    print(synset.definition())\n",
    "    print(70*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive (n): Example: after reaching the desired velocity the drive is cut off\n",
      "    Lemma('drive.n.01.drive')\n",
      "    Lemma('drive.n.01.thrust')\n",
      "    Lemma('drive.n.01.driving_force')\n",
      "------------------------------------------------------------\n",
      "drive (n): Example: a variable speed drive permitted operation through a range of speeds\n",
      "    Lemma('drive.n.02.drive')\n",
      "------------------------------------------------------------\n",
      "campaign (n): Example: he supported populist campaigns\n",
      "    Lemma('campaign.n.02.campaign')\n",
      "    Lemma('campaign.n.02.cause')\n",
      "    Lemma('campaign.n.02.crusade')\n",
      "    Lemma('campaign.n.02.drive')\n",
      "    Lemma('campaign.n.02.movement')\n",
      "    Lemma('campaign.n.02.effort')\n",
      "------------------------------------------------------------\n",
      "driveway (n): Example: they parked in the driveway\n",
      "    Lemma('driveway.n.01.driveway')\n",
      "    Lemma('driveway.n.01.drive')\n",
      "    Lemma('driveway.n.01.private_road')\n",
      "------------------------------------------------------------\n",
      "drive (n): Example: his drive and energy exhausted his co-workers\n",
      "    Lemma('drive.n.05.drive')\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now we display the synonyms, definitions and lemmas.\n",
    "\n",
    "for synset in the_synsets[:max_entries]:\n",
    "    \n",
    "    vals = synset.name().split('.')\n",
    "    print('{0} ({1}): '.format(vals[0], vals[1]), end='')\n",
    "    if synset.examples():\n",
    "        print('Example: {0}'.format(synset.examples()[0]))\n",
    "        \n",
    "    for lma in synset.lemmas():\n",
    "        print('    {0}'.format(lma))\n",
    "\n",
    "    print(60*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Word Similarities\n",
    "\n",
    "We can use wordnet to compute word similarities. The wordnet corpus has tagged tokens, which can be used to compute paths between two words. Shorter paths generally correspond to more similar words. In the following examples, we start with several simple wordnet synonym rings, and compute similarities between these words. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some words\n",
    "man = wn.synset('man.n.01')\n",
    "woman = wn.synset('woman.n.01')\n",
    "horse = wn.synset('horse.n.01')\n",
    "bird = wn.synset('bird.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path Similarity:\n",
      "----------------------------------------\n",
      "man to woman: 0.333\n",
      "man to horse: 0.077\n",
      "man to bird: 0.125\n",
      "woman to woman: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Now we print similarity measures.\n",
    "fmt_str = '{1} to {2}: {0:4.3f}'\n",
    "\n",
    "print('Path Similarity:')\n",
    "print(40*'-')\n",
    "print(fmt_str.format(wn.path_similarity(man, woman), 'man', 'woman'))\n",
    "print(fmt_str.format(wn.path_similarity(man, horse), 'man', 'horse'))\n",
    "print(fmt_str.format(wn.path_similarity(man, bird), 'man', 'bird'))\n",
    "print(fmt_str.format(wn.path_similarity(woman, woman), 'woman', 'woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leacock-Chodorow Similarity:\n",
      "----------------------------------------\n",
      "man to woman: 2.539\n",
      "man to horse: 1.073\n",
      "man to bird: 1.558\n",
      "woman to woman: 3.638\n"
     ]
    }
   ],
   "source": [
    "print('Leacock-Chodorow Similarity:')\n",
    "print(40*'-')\n",
    "print(fmt_str.format(wn.lch_similarity(man, woman), 'man', 'woman'))\n",
    "print(fmt_str.format(wn.lch_similarity(man, horse), 'man', 'horse'))\n",
    "print(fmt_str.format(wn.lch_similarity(man, bird), 'man', 'bird'))\n",
    "print(fmt_str.format(wn.lch_similarity(woman, woman), 'woman', 'woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu-Palmer Similarity:\n",
      "----------------------------------------\n",
      "man to woman: 0.667\n",
      "man to horse: 0.500\n",
      "man to bird: 0.632\n",
      "woman to woman: 1.000\n"
     ]
    }
   ],
   "source": [
    "print('Wu-Palmer Similarity:')\n",
    "print(40*'-')\n",
    "print(fmt_str.format(wn.wup_similarity(man, woman), 'man', 'woman'))\n",
    "print(fmt_str.format(wn.wup_similarity(man, horse), 'man', 'horse'))\n",
    "print(fmt_str.format(wn.wup_similarity(man, bird), 'man', 'bird'))\n",
    "print(fmt_str.format(wn.wup_similarity(woman, woman), 'woman', 'woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used wordnet to compute word similarities. Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the value of `the_word` to a different word, like _ship_ or _throw_. How many entries does the new word have in the wordnet synset?\n",
    "2. Compute the path similarity for a different set of words, like _cat_, _dog_, _boy, and _girl_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Gensim\n",
    "\n",
    "We can use the topic models constructed by using the gensim library to look for similarity. In this case, we build the topic model and use these models to create a similarity matrix. By transforming new text into this vector space, we can compute similarity measures by using the model learned from the original text. In the following Code cells, we build a topic model by using LDA from our course description text. Finally, we use this new model to compute similarity measures between the original text and new sample text.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As a text example, we use the following course description.\n",
    "info_course = ['Advanced Data Science: This class is an asynchronous, ',\n",
    "               'online course. This course will introduce advanced ',\n",
    "               'data science concepts by building ',\n",
    "               'on the foundational concepts presented in the ',\n",
    "               'prerequisite course: Foundations of Data Science. ', \n",
    "               'Students will first learn how to perform more ',\n",
    "               'statistical data exploration and constructing and ',\n",
    "               'evaluating statistical models. Next, students will ',\n",
    "               'learn machine learning techniques including supervised ',\n",
    "               'and unsupervised learning, dimensional reduction, and ',\n",
    "               'cluster finding. An emphasis will be placed on the ',\n",
    "               'practical application of these techniques to ',\n",
    "               'high-dimensional numerical data, time series data, ',\n",
    "               'image data, and text data. Finally, students will ',\n",
    "               'learn to use relational databases and cloud computing ',\n",
    "               'software components such as Hadoop, Spark, and NoSQL ',\n",
    "               'data stores. Students must have access to a fairly ',\n",
    "               'modern computer, ideally that supports hardware ',\n",
    "               'virtualization, on which they can install software.', \n",
    "               'This class is open to sophomores, juniors, seniors ',\n",
    "               'and graduate students in any discipline who have ',\n",
    "               'either taken a previous data science course or ',\n",
    "               'have received instructor permission.']\n",
    "\n",
    "# Simple stop words\n",
    "stop_words = set('for a of the and to in on an'.split())\n",
    "\n",
    "# Parse text into words, make lowercase and remove stop words\n",
    "txts = [[word for word in sentance.lower().split() if word not in stop_words]\n",
    "        for sentance in info_course]\n",
    "\n",
    "# Keep only those words appearing more than once\n",
    "# Easy with a Counter, but need a flat list\n",
    "from collections import Counter\n",
    "frequency = Counter([word for txt in txts for word in txt])\n",
    "\n",
    "# Now grab tokens that appear more than once\n",
    "tokens = [[token for token in txt if frequency[token] > 1]\n",
    "          for txt in txts]\n",
    "\n",
    "from gensim import corpora\n",
    "dict_gensim = corpora.Dictionary(tokens)\n",
    "\n",
    "crps = [dict_gensim.doc2bow(txt) for txt in txts]\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(crps)\n",
    "\n",
    "crps_tfidf = tfidf[crps]\n",
    "\n",
    "lda_gs = models.LdaModel(corpus=crps_tfidf, id2word=dict_gensim, num_topics=5, passes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.73147033332519029), (1, 0.066899613032101776), (2, 0.066689431422646564), (3, 0.068263404515770718), (4, 0.066677217704290678)]\n"
     ]
    }
   ],
   "source": [
    "# Create bag of words vector space representation for sample text.\n",
    "\n",
    "txt = 'Master Data Science'\n",
    "vec_bow = dict_gensim.doc2bow(txt.lower().split())\n",
    "vec_lda = lda_gs[vec_bow]\n",
    "\n",
    "# Display BOW representation\n",
    "print(vec_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute similarity matrix from the topic model\n",
    "\n",
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(lda_gs[crps_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99992508,  0.20426318,  0.23270291,  0.28520346,  0.98996985,\n",
       "        0.22014055,  0.25401837,  0.21876368,  0.24158329,  0.60134262,\n",
       "        0.28373903,  0.28374803,  0.28392529,  0.21921808,  0.98929203,\n",
       "        0.60134262,  0.51379514,  0.60134262,  0.60134262,  0.21971339,\n",
       "        0.23921844,  0.99952495,  0.28356701], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample text values from similarity matrix\n",
    "\n",
    "index[vec_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find similar sentances to given set of words.\n",
    "\n",
    "import operator\n",
    "\n",
    "def find_similar_sentances(mdl, dct, sml_idx, txt, prct = 0.9):\n",
    "    vec_bow = dct.doc2bow(txt.lower().split())\n",
    "    vec_lda = mdl[vec_bow]\n",
    "    \n",
    "    sml = sorted(enumerate(sml_idx[vec_lda]), \\\n",
    "                 key=operator.itemgetter(1), reverse=True)\n",
    "    print('Score| Text')\n",
    "    print(4*'-', '|', 73*'-')\n",
    "    \n",
    "    for idx, val in sml:\n",
    "        if val > prct:\n",
    "            print('{0:4.3f}: {1}'.format(val, info_course[idx]))\n",
    "            print(4*'-', '|', 73*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score| Text\n",
      "---- | -------------------------------------------------------------------------\n",
      "1.000: Advanced Data Science: This class is an asynchronous, \n",
      "---- | -------------------------------------------------------------------------\n",
      "1.000: either taken a previous data science course or \n",
      "---- | -------------------------------------------------------------------------\n",
      "0.990: prerequisite course: Foundations of Data Science. \n",
      "---- | -------------------------------------------------------------------------\n",
      "0.989: learn to use relational databases and cloud computing \n",
      "---- | -------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "find_similar_sentances(lda_gs, dict_gensim, index, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score| Text\n",
      "---- | -------------------------------------------------------------------------\n",
      "1.000: practical application of these techniques to \n",
      "---- | -------------------------------------------------------------------------\n",
      "0.998: statistical data exploration and constructing and \n",
      "---- | -------------------------------------------------------------------------\n",
      "0.997: learn machine learning techniques including supervised \n",
      "---- | -------------------------------------------------------------------------\n",
      "0.709: and unsupervised learning, dimensional reduction, and \n",
      "---- | -------------------------------------------------------------------------\n",
      "0.709: software components such as Hadoop, Spark, and NoSQL \n",
      "---- | -------------------------------------------------------------------------\n",
      "0.709: modern computer, ideally that supports hardware \n",
      "---- | -------------------------------------------------------------------------\n",
      "0.709: virtualization, on which they can install software.\n",
      "---- | -------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "txt = 'evaluate statistical plots'\n",
    "find_similar_sentances(lda_gs, dict_gensim, index, txt, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score| Text\n",
      "---- | -------------------------------------------------------------------------\n",
      "1.000: learn to use relational databases and cloud computing \n",
      "---- | -------------------------------------------------------------------------\n",
      "1.000: prerequisite course: Foundations of Data Science. \n",
      "---- | -------------------------------------------------------------------------\n",
      "0.993: either taken a previous data science course or \n",
      "---- | -------------------------------------------------------------------------\n",
      "0.987: Advanced Data Science: This class is an asynchronous, \n",
      "---- | -------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "txt = 'learn computing'\n",
    "find_similar_sentances(lda_gs, dict_gensim, index, txt, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we build an LDA model from our course description text. Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Try using a different set of words, do the similar sentences make sense? Can you explain why?\n",
    "\n",
    "2. Try using a different corpus (like the twenty newsgroup data set) to build the LDA model. Do word similarities make more or less sense with this new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "Word2vec is a neural network model that was developed several years ago within Google to provide an efficient continuous bag of words and skip-gram algorithms for word-vector representations. By using these approaches, words can be directly compared by using their vector representations to compute similarity measures. The continuous bag of words can be used to predict a context given a word, while a skip gram can be used to predict a word given a context. In this notebook we use the word2vec implementation provided in the gensim library. We first create the model, in this case we start with the parsed course description. Given this model, we can compute word similarities.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(txts, window=2, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.000: data to data\n",
      " 0.127: data to science\n",
      " 0.035: image to data\n",
      " 0.097: students to computing\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity between two words.\n",
    "\n",
    "def get_similarity(mdl, w1, w2):\n",
    "    sml = mdl.similarity(w1, w2)\n",
    "    print(f'{sml:6.3f}: {w1} to {w2}')\n",
    "\n",
    "get_similarity(model, 'data', 'data')\n",
    "get_similarity(model, 'data', 'science')\n",
    "get_similarity(model, 'image', 'data')\n",
    "get_similarity(model, 'students', 'computing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The previous example demonstrated how to use _word2vec_ by using the gensim library. But given the small size of the text document, this didn't really demonstrate the full power of this approach. We now switch to the NLTK movie review corpus, and build a word2vec model from this text data. First, we read the data into the notebook, before tokenizing the data and building the vector space model. Given the large number of words in this corpus, we can compute similarities between a larger number of words, as well as explore relationships between words, all based on the relative occurrences of words in the original corpus.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load movie review corpus\n",
    "import nltk\n",
    "mvr = nltk.corpus.movie_reviews\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "data_dir = home + 'nltk_data/corpora/movie_reviews'\n",
    "mvr = load_files(data_dir, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in Corpus = 2000\n"
     ]
    }
   ],
   "source": [
    "# Tokenize movie reviews by using a word-punctuation tokenizer\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "new_mvr = []\n",
    "\n",
    "# We explicitly cull out punctuation tokens\n",
    "for mvr in mvr.data:\n",
    "    wtks = tokenizer.tokenize(mvr.decode('utf-8'))\n",
    "    new_mvr.append([wtk for wtk in wtks if wtk not in string.punctuation])\n",
    "\n",
    "print(f'Number of reviews in Corpus = {len(new_mvr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word2vec model from movie reviews\n",
    "model = Word2Vec(new_mvr, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.000: girl to girl\n",
      " 0.948: boy to girl\n",
      " 0.892: woman to man\n",
      " 0.912: woman to girl\n"
     ]
    }
   ],
   "source": [
    "# Compute Cosine Similarities from Corpus\n",
    "get_similarity(model, 'girl', 'girl')\n",
    "get_similarity(model, 'boy', 'girl')\n",
    "get_similarity(model, 'woman', 'man')\n",
    "get_similarity(model, 'woman', 'girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple function to display words that are similar to a given word\n",
    "def show_words(vals, type='Cosine Similarity'):\n",
    "    print(f'{\"Word\":14s}: {type}')\n",
    "    print(40*'-')\n",
    "    for val in vals:\n",
    "        print(f'{val[0]:14s}: {val[1]:6.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word          : Cosine Similarity\n",
      "----------------------------------------\n",
      "girl          :  0.948\n",
      "jack          :  0.873\n",
      "woman         :  0.868\n",
      "man           :  0.858\n",
      "girlfriend    :  0.852\n"
     ]
    }
   ],
   "source": [
    "#Compute cosine similarity between two words.\n",
    "\n",
    "vals = model.most_similar('boy', topn=5)\n",
    "show_words(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify words that don't belong\n",
    "\n",
    "wrd_lst = ['boy', 'horse', 'cow', 'pig']\n",
    "\n",
    "model.doesnt_match(wrd_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity =  0.908\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity between two sets of words.\n",
    "\n",
    "val = model.n_similarity(['boy', 'girl'], ['man', 'woman'])\n",
    "\n",
    "print(f'Cosine Similarity = {val:6.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word          : Cosine Similarity\n",
      "----------------------------------------\n",
      "husband       :  0.839\n",
      "mother        :  0.834\n",
      "son           :  0.831\n",
      "boy           :  0.830\n",
      "julia         :  0.820\n"
     ]
    }
   ],
   "source": [
    "# Find similar words (Cosine Similarity)\n",
    "\n",
    "vals = model.most_similar(positive=['woman', 'girl'], \n",
    "                          negative=['man'], topn=5)\n",
    "show_words(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word          : CosMul Similarity\n",
      "----------------------------------------\n",
      "actress       :  0.929\n",
      "oscar         :  0.921\n",
      "performance   :  0.905\n",
      "role          :  0.896\n",
      "nomination    :  0.892\n"
     ]
    }
   ],
   "source": [
    "# Find similar words (Multiplicative Combination method)\n",
    "\n",
    "vals = model.most_similar_cosmul('actor', topn=5)\n",
    "show_words(vals, 'CosMul Similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we applied the word2vec model to the movie review data set. Before proceeding, do the results make sense (feel free to discuss in the class forums). Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the `count` and `window` values. How do the results change?\n",
    "2. Try exploring the relationship between other words, like _cat_, _dog_, _bird_, and _horse_; or other word combinations that are likely to appear in movie reviews.\n",
    "3. Can you apply word2vec to a different corpus, like the Brown corpus in NLTK? How do the similarity measures change with this new corpus for the same set of words?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. Wikipedia article on [Vector Space Models][wvsm]\n",
    "1. Wikipedia article on [Latent Semantic Analysis (LSA)][wlsa] \n",
    "1. Wikipedia article on [Semantic Similarity][wss]\n",
    "1. Blog discussing the use of [semantic analysis for fashion][bwe] \n",
    "1. Implementation of [word2vec in python][wip], via gensim\n",
    "1. Blog article discussing the use of [word2vec for text analysis][wta]\n",
    "1. Original Google [word2vec][gw2v] implementation. \n",
    "\n",
    "-----\n",
    "\n",
    "[bwe]: http://developers.lyst.com/2014/11/11/word-embeddings-for-fashion/\n",
    "\n",
    "[wvsm]: https://en.wikipedia.org/wiki/Vector_space_model\n",
    "[ww2v]: https://en.wikipedia.org/wiki/Word2vec\n",
    "[wlsa]: https://en.wikipedia.org/wiki/Latent_semantic_analysis\n",
    "[wss]: https://en.wikipedia.org/wiki/Semantic_similarity\n",
    "\n",
    "[gw2v]: https://code.google.com/archive/p/word2vec/\n",
    "[wip]: http://radimrehurek.com/gensim/models/word2vec.html\n",
    "[wta]: http://blog.dato.com/practical-text-analysis-using-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
