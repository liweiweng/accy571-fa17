{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP: Semantic Analysis\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most elusive goals in natural language processing is the identification of semantic meaning from text. In this case, we don't want to simply use text to identify topics or classify documents. Instead we want to use the relationships between words in documents to gain insight from context. One simple way to understand this concept is to see that when words occur in similar arrangements or patterns across documents, the pattern conveys meaning beyond the simple appearance of the words themselves. \n",
    "\n",
    "In this notebook, we explore semantic analysis. First we use the wordnet corpus to identify similar words based on pre-defined relationships. Second, we use the gensim library to create topic models that can be used to compute similarity measures based on the inherent patterns of words within a corpus. Finally, we explore the word2vec approach, where a neural network is trained on a large corpus to identify relationships between words and phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[N-Grams](#N-Grams)\n",
    "\n",
    "[N-Gram Classification](#N-Gram-Classification)\n",
    "\n",
    "[Stemming](#Stemming)\n",
    "\n",
    "[Clustering Analysis](#Clustering-Analysis)\n",
    "\n",
    "[Dimension-Reduction](#Dimension-Reduction)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first include the notebook setup code and we define our _home_ directory.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we find our HOME directory\n",
    "home_dir = !echo $HOME\n",
    "\n",
    "# Define data directory\n",
    "home = home_dir[0] +'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Wordnet\n",
    "\n",
    "[Wordnet][wdn] is an English lexical database that groups words into\n",
    "synsets, which is shorthand for synonym sets. The database was created\n",
    "under at Princeton University and has been distributed with an open\n",
    "license. Given its nature, it is a different corpus than the Treebank or\n",
    "Brown corpora analyzed in the [Topic Modeling][l2] notebook. A wordnet\n",
    "entry can have multiple definitions for a word, associated synonyms,\n",
    "lemmas, and other information that can be used to algorithmically\n",
    "identify relationships between words. In the next few code cells we\n",
    "explore the wordnet corpus, before moving on to using it to compute word\n",
    "similarities.\n",
    "\n",
    "-----\n",
    "[wdn]: https://en.wikipedia.org/wiki/WordNet \n",
    "[l2]: intro2nlp-tm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/data_scientist/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/' not found.  Please use\n  the NLTK Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/data_scientist/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f6522cf7ab80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Choose a word, change this to see different results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mthe_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'drive'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mthe_synsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Display summary stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/data_scientist/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "# Explore WordNet synonym rings\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# We limit the number of items in ring to display\n",
    "max_entries = 5\n",
    "\n",
    "# Choose a word, change this to see different results.\n",
    "the_word = 'drive'\n",
    "the_synsets = wn.synsets(the_word)\n",
    "\n",
    "# Display summary stats\n",
    "print('{0} total entries in synonym ring for {1}. '.format(len(the_synsets), the_word), end='')\n",
    "print('Only showing top {0} entries.'.format(max_entries))\n",
    "print(70*'-')\n",
    "\n",
    "# Step through ring and display data\n",
    "for synset in the_synsets[:max_entries]:\n",
    "    vals = synset.name().split('.')\n",
    "    print('{0} ({1}): '.format(vals[0], vals[1]), end='')\n",
    "    print(synset.definition())\n",
    "    print(70*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we display the synonyms, definitions and lemmas.\n",
    "\n",
    "for synset in the_synsets[:max_entries]:\n",
    "    \n",
    "    vals = synset.name().split('.')\n",
    "    print('{0} ({1}): '.format(vals[0], vals[1]), end='')\n",
    "    if synset.examples():\n",
    "        print('Example: {0}'.format(synset.examples()[0]))\n",
    "        \n",
    "    for lma in synset.lemmas():\n",
    "        print('    {0}'.format(lma))\n",
    "\n",
    "    print(60*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Word Similarities\n",
    "\n",
    "We can use wordnet to compute word similarities. The wordnet corpus has\n",
    "tagged tokens, which can be used to compute paths between two words.\n",
    "Shorter paths generally correspond to more similar words. In the\n",
    "following examples, we start with several simple wordnet synonym rings,\n",
    "and compute similarities between these words.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some words\n",
    "man = wn.synset('man.n.01')\n",
    "woman = wn.synset('woman.n.01')\n",
    "horse = wn.synset('horse.n.01')\n",
    "bird = wn.synset('bird.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we print similarity measures.\n",
    "fmt_str = '{1} to {2}: {0:4.3f}'\n",
    "\n",
    "print('Path Similarity:')\n",
    "print(40*'-')\n",
    "print(fmt_str.format(wn.path_similarity(man, woman), 'man', 'woman'))\n",
    "print(fmt_str.format(wn.path_similarity(man, horse), 'man', 'horse'))\n",
    "print(fmt_str.format(wn.path_similarity(man, bird), 'man', 'bird'))\n",
    "print(fmt_str.format(wn.path_similarity(woman, woman), 'woman', 'woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Leacock-Chodorow Similarity:')\n",
    "print(40*'-')\n",
    "print(fmt_str.format(wn.lch_similarity(man, woman), 'man', 'woman'))\n",
    "print(fmt_str.format(wn.lch_similarity(man, horse), 'man', 'horse'))\n",
    "print(fmt_str.format(wn.lch_similarity(man, bird), 'man', 'bird'))\n",
    "print(fmt_str.format(wn.lch_similarity(woman, woman), 'woman', 'woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Wu-Palmer Similarity:')\n",
    "print(40*'-')\n",
    "print(fmt_str.format(wn.wup_similarity(man, woman), 'man', 'woman'))\n",
    "print(fmt_str.format(wn.wup_similarity(man, horse), 'man', 'horse'))\n",
    "print(fmt_str.format(wn.wup_similarity(man, bird), 'man', 'bird'))\n",
    "print(fmt_str.format(wn.wup_similarity(woman, woman), 'woman', 'woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used wordnet to compute word similarities.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Change the value of `the_word` to a different word, like _ship_ or\n",
    "_throw_. How many entries does the new word have in the wordnet synset?\n",
    "2. Compute the path similarity for a different set of words, like\n",
    "_cat_, _dog_, _boy, and _girl_.\n",
    "\n",
    "----------\n",
    "\n",
    "## Gensim\n",
    "\n",
    "We can use the topic models constructed by using the gensim library to\n",
    "look for similarity. In this case, we build the topic model and use\n",
    "these models to create a similarity matrix. By transforming new text\n",
    "into this vector space, we can compute similarity measures by using the\n",
    "model learned from the original text. In the following code cells, we\n",
    "build a topic model by using LDA from our course description text.\n",
    "Finally, we use this new model to compute similarity measures between\n",
    "the original text and new sample text.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a text example, we use the course description for INFO490  SP16.\n",
    "info_course = ['Advanced Data Science: This class is an asynchronous, online course.', \n",
    "               'This course will introduce advanced data science concepts by building on the foundational concepts presented in INFO 490: Foundations of Data Science.', \n",
    "               'Students will first learn how to perform more statistical data exploration and constructing and evaluating statistical models.', \n",
    "               'Next, students will learn machine learning techniques including supervised and unsupervised learning, dimensional reduction, and cluster finding.', \n",
    "               'An emphasis will be placed on the practical application of these techniques to high-dimensional numerical data, time series data, image data, and text data.', \n",
    "               'Finally, students will learn to use relational databases and cloud computing software components such as Hadoop, Spark, and NoSQL data stores.', \n",
    "               'Students must have access to a fairly modern computer, ideally that supports hardware virtualization, on which they can install software.', \n",
    "               'This class is open to sophomores, juniors, seniors and graduate students in any discipline who have either taken a previous INFO 490 data science course or have received instructor permission.']\n",
    "\n",
    "# Simple stop words\n",
    "stop_words = set('for a of the and to in on an'.split())\n",
    "\n",
    "# Parse text into words, make lowercase and remove stop words\n",
    "txts = [[word for word in sentance.lower().split() if word not in stop_words]\n",
    "        for sentance in info_course]\n",
    "\n",
    "# Keep only those words appearing more than once\n",
    "# Easy with a Counter, but need a flat list\n",
    "from collections import Counter\n",
    "frequency = Counter([word for txt in txts for word in txt])\n",
    "\n",
    "# Now grab tokens that appear more than once\n",
    "tokens = [[token for token in txt if frequency[token] > 1]\n",
    "          for txt in txts]\n",
    "\n",
    "from gensim import corpora\n",
    "dict_gensim = corpora.Dictionary(tokens)\n",
    "\n",
    "crps = [dict_gensim.doc2bow(txt) for txt in txts]\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(crps)\n",
    "\n",
    "crps_tfidf = tfidf[crps]\n",
    "\n",
    "lda_gs = models.LdaModel(corpus=crps_tfidf, id2word=dict_gensim, num_topics=5, passes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words vector space representation for sample text.\n",
    "\n",
    "txt = 'Master Data Science'\n",
    "vec_bow = dict_gensim.doc2bow(txt.lower().split())\n",
    "vec_lda = lda_gs[vec_bow]\n",
    "\n",
    "# Display BOW representation\n",
    "print(vec_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix from the topic model\n",
    "\n",
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(lda_gs[crps_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample text values from similarity matrix\n",
    "\n",
    "index[vec_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar sentances to given set of words.\n",
    "\n",
    "import operator\n",
    "\n",
    "def find_similar_sentances(mdl, dct, sml_idx, txt, prct = 0.9):\n",
    "    vec_bow = dct.doc2bow(txt.lower().split())\n",
    "    vec_lda = mdl[vec_bow]\n",
    "    \n",
    "    sml = sorted(enumerate(sml_idx[vec_lda]), \\\n",
    "                 key=operator.itemgetter(1), reverse=True)\n",
    "    print('Score| Text')\n",
    "    print(4*'-', '|', 73*'-')\n",
    "    \n",
    "    for idx, val in sml:\n",
    "        if val > prct:\n",
    "            print('{0:4.3f}: {1}'.format(val, info_course[idx]))\n",
    "            print(4*'-', '|', 73*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_sentances(lda_gs, dict_gensim, index, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'evaluate statistical plots'\n",
    "find_similar_sentances(lda_gs, dict_gensim, index, txt, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'learn computing'\n",
    "find_similar_sentances(lda_gs, dict_gensim, index, txt, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we build an LDA model from our course\n",
    "description text. Now that you have run the Notebook, go back and make\n",
    "the following changes to see how the results change.\n",
    "\n",
    "1. Try using a different set of words, do the similar sentences make\n",
    "sense? Can you explain why?\n",
    "\n",
    "2. Try using a different corpus (like the twenty newsgroup data set) to\n",
    "build the LDA model. Do word similarities make more or less sense with\n",
    "this new model\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "Word2vec is a neural network model that was developed several years ago\n",
    "within Google to provide an efficient continuous bag of words and\n",
    "skip-gram algorithms for word-vector representations. By using these\n",
    "approaches, words can be directly compared by using their vector\n",
    "representations to compute similarity measures. The continuous bag of\n",
    "words can be used to predict a context given a word, while a skip gram\n",
    "can be used to predict a word given a context. In this notebook we use\n",
    "the word2vec implementation provided in the gensim library. We first\n",
    "create the model, in this case we start with the parsed course\n",
    "description. Given this model, we can compute word similarities.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(txts, window=2, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between two words.\n",
    "\n",
    "def get_similarity(mdl, w1, w2):\n",
    "    sml = mdl.similarity(w1, w2)\n",
    "    print('{0:6.3f}: {1} to {2}'.format(sml, w1, w2))\n",
    "\n",
    "get_similarity(model, 'data', 'data')\n",
    "get_similarity(model, 'data', 'science')\n",
    "get_similarity(model, 'image', 'data')\n",
    "get_similarity(model, 'students', 'computing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The previous example demonstrated how to use _word2vec_ by using the\n",
    "gensim library. But given the small size of the text document, this\n",
    "didn't really demonstrate the full power of this approach. We now switch\n",
    "to the NLTK movie review corpus, and build a word2vec model from this\n",
    "text data. First, we read the data into the notebook, before tokenizing\n",
    "the data and building the vector space model. Given the large number of\n",
    "words in this corpus, we can compute similarities between a larger\n",
    "number of words, as well as explore relationships between words, all\n",
    "based on the relative occurrences of words in the original corpus.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movie review corpus\n",
    "import nltk\n",
    "mvr = nltk.corpus.movie_reviews\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "data_dir = '/home/data_scientist/data/nltk_data/corpora/movie_reviews'\n",
    "mvr = load_files(data_dir, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize movie reviews by using a word-punctuation tokenizer\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "new_mvr = []\n",
    "\n",
    "# We explicitly cull out punctuation tokens\n",
    "for mvr in mvr.data:\n",
    "    wtks = tokenizer.tokenize(mvr.decode('utf-8'))\n",
    "    new_mvr.append([wtk for wtk in wtks if wtk not in string.punctuation])\n",
    "\n",
    "print('Number of reviews in Corpus = {0}'.format(len(new_mvr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build word2vec model from movie reviews\n",
    "model = gensim.models.Word2Vec(new_mvr, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cosine Similarities from Corpus\n",
    "get_similarity(model, 'girl', 'girl')\n",
    "get_similarity(model, 'boy', 'girl')\n",
    "get_similarity(model, 'woman', 'man')\n",
    "get_similarity(model, 'woman', 'girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple function to display words that are similar to a given word\n",
    "def show_words(vals, type='Cosine Similarity'):\n",
    "    print('{0:14s}: {1}'.format('Word', type))\n",
    "    print(40*'-')\n",
    "    for val in vals:\n",
    "        print('{0:14s}: {1:6.3f}'.format(val[0], val[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute cosine similarity between two words.\n",
    "\n",
    "vals = model.most_similar('boy', topn=5)\n",
    "show_words(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify words that don't belong\n",
    "\n",
    "wrd_lst = ['boy', 'horse', 'cow', 'pig']\n",
    "\n",
    "model.doesnt_match(wrd_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between two sets of words.\n",
    "\n",
    "val = model.n_similarity(['boy', 'girl'], ['man', 'woman'])\n",
    "\n",
    "print('Cosine Similarity = {0:6.3f}'.format(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words (Cosine Similarity)\n",
    "\n",
    "vals = model.most_similar(positive=['woman', 'girl'], negative=['man'], topn=5)\n",
    "show_words(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words (Multiplicative Combination method)\n",
    "\n",
    "vals = model.most_similar_cosmul('actor', topn=5)\n",
    "show_words(vals, 'CosMul Similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we applied the word2vec model to the movie\n",
    "review data set. Before proceeding, do the results make sense (feel free\n",
    "to discuss in the class forums). Now that you have run the Notebook, go\n",
    "back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the `count` and `window` values. How do the results change?\n",
    "2. Try exploring the relationship between other words, like _cat_,\n",
    "_dog_, _bird_, and _horse_; or other word combinations that are likely\n",
    "to appear in movie reviews.\n",
    "3. Can you apply word2vec to a different corpus, like the Brown corpus\n",
    "in NLTK? How do the similarity measures change with this new corpus for\n",
    "the same set of words?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used feature selection to identify the most important features in our simple classification pipeline. Now that you have run the Notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the vectorizer to change the case of all words an to employ stemming. How do the results (tokens) change?\n",
    "\n",
    "2. Change the classification algorithm to a more accurate method. How do the results change? How does the computational time change?\n",
    "\n",
    "Finally, what do the list of tokens say about the fact we did not remove headers or footers from the newsgroup postings? Feel free to comment on these questions in the course forum.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. [XML Tutorial][1] by W3Schools\n",
    "3. [SVG Tutorial][3] by W3Schools\n",
    "4. The [ColorBrewer2][cb2] website\n",
    "\n",
    "-----\n",
    "\n",
    "[1]: http://www.w3schools.com/xml/default.asp\n",
    "[3]: http://www.w3schools.com/svg/default.asp\n",
    "[cb2]: http://colorbrewer2.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
