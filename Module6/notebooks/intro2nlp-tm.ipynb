{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP: Topic Modeling\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing large text corpora, trends can appear. These trends can be repeated use of common phrases or terms that are indicative of common underlying themes or topics. For example, books on programming might refer to themes such as human computer interaction, optimization and performance, or identifying and removing error conditions. Finding these common topics can be important for a number of reasons. On the one hand, when they are completely unknown, they can be used to provide new insight into text documents. On the other hand, when they may be partially or even completely unknown, computationally identified topics can provide deeper or more concise insight into the relationship between documents.\n",
    "\n",
    "The process of identifying these common topics is known as topic modeling, which is generally a form of unsupervised learning. As a specific example, consider the [twenty newsgroup][tw] data that we have analyzed in scikit learn. While there are twenty different newsgroups, it turns out they can be grouped into six related categories: computers, sports, science, politics, religion, and miscellaneous. While we now these topics ahead of time (from the newsgroup titles), we can apply topic modeling to these data to identify the common words or phrases that define these common topics.\n",
    "\n",
    "In the rest of this notebook, we explore the concept of topic modeling. First we will use the scikit learn library to perform topic modeling. We will introduce and use non-negative matrix factorization and Latent Dirchlet allocation. We apply topic modeling to a text classification problem, and also explore the terms that make up identified topics. Finally, we introduce the gensim library, which provides additional techniques for topic modeling.\n",
    "\n",
    "-----\n",
    "\n",
    "[tw]: http://qwone.com/~jason/20Newsgroups/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[N-Grams](#N-Grams)\n",
    "\n",
    "[N-Gram Classification](#N-Gram-Classification)\n",
    "\n",
    "[Stemming](#Stemming)\n",
    "\n",
    "[Clustering Analysis](#Clustering-Analysis)\n",
    "\n",
    "[Dimension-Reduction](#Dimension-Reduction)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first include the notebook setup code and we define our _home_ directory.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we find our HOME directory\n",
    "home_dir = !echo $HOME\n",
    "\n",
    "# Define data directory\n",
    "home = home_dir[0] +'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', \n",
    "                           subset='train', shuffle=True, random_state=23,\n",
    "                           remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "test = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', \n",
    "                          subset='test', shuffle=True, random_state=23,\n",
    "                          remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TD-IDF on newgroup data.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cv = TfidfVectorizer(stop_words = 'english',\n",
    "                     lowercase=True,\n",
    "                     min_df=2,\n",
    "                     max_features=5000)\n",
    "                     \n",
    "train_data = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Non-Negative Matrix Factorization\n",
    "\n",
    "We can apply [non-negative matrix factorization][wnmf] (NMF) to compute\n",
    "topics in a corpus. We start with a term-document matrix, which we\n",
    "factor in to a term-feature and a feature-document matrices. The latter\n",
    "matrix can be used to identify data clusters (or topics) in the corpus.\n",
    "We demonstrate the use of NMF to perform topic modeling by using the\n",
    "scikit learn library's [NMF implementation][sknmf]. \n",
    "\n",
    "-----\n",
    "\n",
    "[wnmf]: https://en.wikipedia.org/wiki/Non-negative_matrix_factorization\n",
    "[sknmf]: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute topics by using NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "num_topics = 6\n",
    "nmf = NMF(n_components = num_topics, max_iter = 1000).fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helper_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-75a9adbfe0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper_code\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtpterms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmf_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helper_code'"
     ]
    }
   ],
   "source": [
    "from helper_code import tpterms as tp\n",
    "\n",
    "nmf_topics = tp.get_topics(cv, nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Understanding Topic Terms\n",
    "\n",
    "We can explore the terms that are important for each topic by creating a\n",
    "DataFrame to map our topic terms to the original twenty newsgroups. We\n",
    "demonstrate this below by first normalizing the transformed data to have\n",
    "unit probability. We use these data to create the DataFrame and group\n",
    "the resulting rows by the associated newsgroup as shown below.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform and normalize the data, \n",
    "# by using l1 so document topic probabilty sums to unity.\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "td = nmf.transform(train_data)\n",
    "td_norm = normalize(td, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use a DataFrame to simplify the collecting of the data for display.\n",
    "\n",
    "df = pd.DataFrame(td_norm, columns=nmf_topics)\n",
    "df.fillna(value=0, inplace=True)\n",
    "df['label'] = pd.Series(train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now group and add human names for the labels\n",
    "df_lbl = df.groupby('label').mean()\n",
    "df_lbl['Names'] = pd.Series(train['target_names'], dtype=\"category\")\n",
    "\n",
    "# Now display the grouped data\n",
    "df_lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Topic-based Classification\n",
    "\n",
    "If documents are composed of topics, we can leverage defined topics to\n",
    "classify new documents based on the topics that are assigned to each new\n",
    "document. In the following code cells, we first train a Naive Bayes\n",
    "classifier on the topics in the training data sample of the twenty\n",
    "newsgroup data set. We compute the topics, by using the previously\n",
    "created NMF model, for the test data and compute classifications from\n",
    "these topic models. Finally, the resulting classification report and\n",
    "confusion matrix are shown to demonstrate the quality of this\n",
    "classification method.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classifier from topics.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(td, train['target'])\n",
    "\n",
    "# Apply classifier to blind test data\n",
    "ts_preds = clf.predict(nmf.transform(test_data))\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(test['target'], ts_preds,\n",
    "    target_names = test['target_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Confusion Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "from helper_code import mlplots as mp\n",
    "mp.confusion(test['target'], ts_preds, range(20), 20, 'Naive Bayes Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we introduced basic topic modeling by using the\n",
    "scikit learn library and employed NMF in a text classification pipeline.\n",
    "Now that you have run the Notebook, why do you think the results from\n",
    "the topic model-based classification are so poor, especially when\n",
    "compared to the same algorithm without topic modeling (feel free to\n",
    "discuss this in the class forum)?\n",
    "\n",
    "Try making the following changes:\n",
    "\n",
    "1. Increase the number of topics from six to sixty. Do the results change?\n",
    "2. Change the classification algorithm to a random forest. Do the results\n",
    "change?\n",
    "3. Try changing the TFIDF parameters to use more features and n-grams. Do\n",
    "the results change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Latent Dirichlet allocation\n",
    "\n",
    "Perhaps the most popular topic modeling algorithm is [Latent Dirichlet\n",
    "allocation][wlda] or LDA. LDA assumes that documents in a Corpus result\n",
    "from a mixture of a small number of topics,  and that the words in the\n",
    "document can be attributed to one of the topics that make up that\n",
    "document. The scikit learn library has an [LDA implementation][sklda],\n",
    "which can be easily applied to a data set, as demonstrated below. After\n",
    "constructing an LDA model, we extract the topics (in this case we are\n",
    "identifying topics for the newsgroup data set) and display the top terms\n",
    "in each topic.\n",
    "\n",
    "-----\n",
    "[wlda]: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "[sklda]: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=num_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=5.,\n",
    "                                random_state=23).fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = tp.get_topics(cv, lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Visualizing Topics\n",
    "\n",
    "We can visualize he important terms in a topic by constructing a\n",
    "[wordle][ww], which is a word cloud where the size of the word indicates\n",
    "its relative importance. To do this, we will use the Python [word cloud][pwn]\n",
    "library as demonstrated in the following code cell, where we display a\n",
    "word cloud for the first topic and a word cloud for all topics.\n",
    "\n",
    "-----\n",
    "[ww]: http://www.wordle.net\n",
    "[pwn]: http://amueller.github.io/word_cloud/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_code import wcviz as wc\n",
    "\n",
    "wc.make_wc(lda_topics[0].replace(',', ''), 'Words from Topic 0')\n",
    "\n",
    "text = ', '.join(lda_topics)\n",
    "wc.make_wc(text.replace(',', ''), 'Words from all Topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "\n",
    "In the preceding cells, we introduced Latent Dirichlet allocation. Now\n",
    "that you have run the Notebook, try to use LDA in the previous text\n",
    "classification problem. Are the results better or worse? Can you explain\n",
    "why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Gensim\n",
    "\n",
    "While NLTK is a useful library to learn the basic concepts in text\n",
    "analysis and natural language processing, there are other libraries that\n",
    "provide powerful NLP functionality. One of the most important libraries\n",
    "in this category is the [_gensim_ library][gl], which is an open source,\n",
    "Python library to create vector-space models for text data that can be\n",
    "used to create topic models. In the following section, we review how to\n",
    "use the gensim library to perform basic text analysis, before learning\n",
    "how to use gensim  to create topic models. \n",
    "\n",
    "An important task in gensim is to the creation of the vector space model\n",
    "for a text document. The indices into the vector space are mapped to the\n",
    "actual terms (or words) by a dictionary; thus we will need the actual\n",
    "vector space model and this dictionary to use gensim for topic\n",
    "modeling. These concepts are demonstrated in the following few code\n",
    "cells, where we analyze the course description.\n",
    "\n",
    "-----\n",
    "[gl]: http://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next section follows gensim tutorial\n",
    "\n",
    "# As a text example, we use the course description for INFO490  SP16.\n",
    "info_course = ['Advanced Data Science: This class is an asynchronous, online course.', \n",
    "               'This course will introduce advanced data science concepts by building on the foundational concepts presented in INFO 490: Foundations of Data Science.', \n",
    "               'Students will first learn how to perform more statistical data exploration and constructing and evaluating statistical models.', \n",
    "               'Next, students will learn machine learning techniques including supervised and unsupervised learning, dimensional reduction, and cluster finding.', \n",
    "               'An emphasis will be placed on the practical application of these techniques to high-dimensional numerical data, time series data, image data, and text data.', \n",
    "               'Finally, students will learn to use relational databases and cloud computing software components such as Hadoop, Spark, and NoSQL data stores.', \n",
    "               'Students must have access to a fairly modern computer, ideally that supports hardware virtualization, on which they can install software.', \n",
    "               'This class is open to sophomores, juniors, seniors and graduate students in any discipline who have either taken a previous INFO 490 data science course or have received instructor permission.']\n",
    "\n",
    "# Simple stop words\n",
    "stop_words = set('for a of the and to in on an'.split())\n",
    "\n",
    "# Parse text into words, make lowercase and remove stop words\n",
    "txts = [[word for word in sentance.lower().split() if word not in stop_words]\n",
    "        for sentance in info_course]\n",
    "\n",
    "# Keep only those words appearing more than once\n",
    "# Easy with a Counter, but need a flat list\n",
    "from collections import Counter\n",
    "frequency = Counter([word for txt in txts for word in txt])\n",
    "\n",
    "# Now grab tokens that appear more than once\n",
    "tokens = [[token for token in txt if frequency[token] > 1]\n",
    "          for txt in txts]\n",
    "\n",
    "# Display the tokens\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "\n",
    "pp.pprint(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dictionary mapptin for given text corpus\n",
    "\n",
    "from gensim import corpora\n",
    "dict_gensim = corpora.Dictionary(tokens)\n",
    "print(dict_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mapping between index and word in Bag of Word model.\n",
    "\n",
    "print(dict_gensim.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample text string as a bag of words.\n",
    "\n",
    "new_txt = 'data science is cool, you should take this course to learn data concepts'\n",
    "new_vec = dict_gensim.doc2bow(new_txt.lower().split())\n",
    "pp.pprint(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display corpus as bag of words.\n",
    "\n",
    "crps = [dict_gensim.doc2bow(txt) for txt in txts]\n",
    "print(crps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Topic Modeling with gensim\n",
    "\n",
    "We can use the gensim library to perform topic modeling. We first\n",
    "transform our info text document to a TFIDF model. The gensim library\n",
    "requires a dictionary to map indices into the TFIDF model to the words,\n",
    "which we can do with our `dict_gensim` object. In the next few code\n",
    "cells, we first create our TFIDF document matrix, display the a sample\n",
    "text string, the bag of words model for this text strings, and the TFIDF\n",
    "model of this document. Next, we construct an Latent Dirchlet allocation\n",
    "model of this document using our dictionary mapping object. Finally, we\n",
    "display the topics, before quantifying the top topic for each sentence\n",
    "in our original corpus.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(crps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sentance, bago of words model, and TFIDF representation.\n",
    "\n",
    "print(new_txt)\n",
    "print(new_vec)\n",
    "\n",
    "pp.pprint(tfidf[new_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LDA model for corpus\n",
    "\n",
    "crps_tfidf = tfidf[crps]\n",
    "lda_gs = models.LdaModel(corpus=crps_tfidf, id2word=dict_gensim, num_topics=3, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display topics as functions over their top terms\n",
    "\n",
    "lda_gs.print_topics(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine primary topic for each sentance in original text\n",
    "import operator\n",
    "\n",
    "for idx, txt in enumerate(lda_gs[crps_tfidf]):\n",
    "    srt_txt = sorted(txt, key=operator.itemgetter(1))\n",
    "    print('Sentance {0:1d} has primary topic {1:1d} with probability = {2:4.3f}'\\\n",
    "          .format(idx, srt_txt[-1][0], srt_txt[-1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttps = lda_gs.top_topics(corpus=crps_tfidf, num_words=5)\n",
    "idx = 0\n",
    "\n",
    "for lst, val in ttps:\n",
    "    print('Topic {0}'.format(idx))\n",
    "    print(35*('-'))\n",
    "    idx += 1\n",
    "    for i, z in lst:\n",
    "        print('    {0:20s}: {1:5.4f}'.format(z, i))\n",
    "    print(35*('-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Topic Modeling with gensim\n",
    "\n",
    "We can use the gensim library to perform topic modeling of the twenty\n",
    "newsgroup data. We first need to transform a sparse matrix (as provided\n",
    "by the scikit learn library) into a gensim corpus. We also need to\n",
    "construct a vocabulary dictionary, which we can do by transforming the\n",
    "scikit learn `CountVectorizer` vocabulary into a dictionary that maps\n",
    "between `id` and the `word`. We demonstrate this transformation in the\n",
    "following code cell for the newsgroup training data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils as mat\n",
    "from gensim import models as md\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# transform sparse matrix into gensim corpus\n",
    "td_gensim = mat.Sparse2Corpus(train_data, documents_columns=False)\n",
    "\n",
    "# Build temporary dictionary from scikit learn vectorizer\n",
    "# for use with gensim\n",
    "tmp_dct = dict((idv, word) for word, idv in cv.vocabulary_.items())\n",
    "dct = Dictionary.from_corpus(td_gensim, id2word=tmp_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Latent Semantic Analysis\n",
    "\n",
    "We can use the gensim library to perform [Latent Semantic\n",
    "Analysis][wlsa] or LSA; in gensim, however, this technique is called\n",
    "[Latent Semantic Indexing][glsi] (or LSI). LSA assumes that words with\n",
    "similar meanings will occur in close proximity. By leveraging this\n",
    "assumption, we can build and process a term document matrix. After\n",
    "processing, a cosine similarity can be used to identify words that are\n",
    "similar. This technique is applied in the following code cell, where we\n",
    "build an LSA model with six topics from the newsgroup text. The topics\n",
    "are subsequently displayed as functions of the most important terms in\n",
    "each topic.\n",
    "\n",
    "-----\n",
    "[wlsa]: https://en.wikipedia.org/wiki/Latent_semantic_analysis\n",
    "[glsi]: http://radimrehurek.com/gensim/models/lsimodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI\n",
    "\n",
    "lsi = md.lsimodel.LsiModel(corpus=td_gensim, id2word=dct, num_topics=6)\n",
    "lsi.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Latent Dirichlet allocation\n",
    "\n",
    "The gensim library also provides an implementation of the [Latent\n",
    "Dirichlet allocation][glda] or LDA. We demonstrate the gensim LDA\n",
    "technique in the following code cell, where we once again create an LDA\n",
    "model with six topics for the newsgroup text. We subsequently display\n",
    "the topics as functions of the top words within each topic. Finally, we\n",
    "display the top five words in each topic, along with their topic\n",
    "coherence, which is a measure of the words importance to the specific\n",
    "topic.\n",
    "\n",
    "-----\n",
    "\n",
    "[glda]: http://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "\n",
    "lda_gs = md.LdaModel(corpus=td_gensim, id2word=dct, num_topics=6, passes=2)\n",
    "lda_gs.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ttps = lda_gs.top_topics(corpus=td_gensim, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "for lst, val in ttps:\n",
    "    print('Topic {0}'.format(idx))\n",
    "    print(35*('-'))\n",
    "    idx += 1\n",
    "    for i, z in lst:\n",
    "        print('    {0:20s}: {1:5.4f}'.format(z, i))\n",
    "    print(35*('-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "\n",
    "In the preceding cells, we used the gensim library to perform topic\n",
    "modeling on the twenty newsgroup data set. Now that you have n the\n",
    "Notebook, try making the following changes.\n",
    "\n",
    "1. Increase the number of topics, how do the results change?\n",
    "2. Can you map the topics to the original newsgroups?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used feature selection to identify the most important features in our simple classification pipeline. Now that you have run the Notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the vectorizer to change the case of all words an to employ stemming. How do the results (tokens) change?\n",
    "\n",
    "2. Change the classification algorithm to a more accurate method. How do the results change? How does the computational time change?\n",
    "\n",
    "Finally, what do the list of tokens say about the fact we did not remove headers or footers from the newsgroup postings? Feel free to comment on these questions in the course forum.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. [XML Tutorial][1] by W3Schools\n",
    "3. [SVG Tutorial][3] by W3Schools\n",
    "4. The [ColorBrewer2][cb2] website\n",
    "\n",
    "-----\n",
    "\n",
    "[1]: http://www.w3schools.com/xml/default.asp\n",
    "[3]: http://www.w3schools.com/svg/default.asp\n",
    "[cb2]: http://colorbrewer2.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
